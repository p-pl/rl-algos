{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a951a47f-88f5-40f0-8fe3-5495f76633b5",
   "metadata": {
    "id": "a951a47f-88f5-40f0-8fe3-5495f76633b5"
   },
   "source": [
    "# Install needed deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c89b3-be5c-49c6-bb0d-4fea729bddee",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Don't forget to run ```apt-get update --fix-missing && sudo apt-get install build-essential``` and ```apt-get install zlib1g-dev``` in case you are running on an Ubuntu image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5fd9d6-8822-43d3-87ad-36ad1f95794b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "bede9272-b231-4d56-97ef-e2e180cf0655",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "82aa9af4-5352-4e8b-dafa-83b334765f71",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: tensortrade in /opt/conda/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /opt/conda/lib/python3.9/site-packages (from scipy) (1.19.5)\n",
      "Requirement already satisfied: gym>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (0.21.0)\n",
      "Requirement already satisfied: plotly>=4.5.0 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (5.7.0)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (2.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1.2 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (6.0)\n",
      "Requirement already satisfied: stochastic>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (0.6.0)\n",
      "Requirement already satisfied: pandas>=0.25.0 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (1.4.2)\n",
      "Requirement already satisfied: ipython>=7.12.0 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (8.2.0)\n",
      "Requirement already satisfied: matplotlib>=3.1.1 in /opt/conda/lib/python3.9/site-packages (from tensortrade) (3.5.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from gym>=0.14.0->tensortrade) (2.0.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (62.1.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (3.0.29)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (2.11.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython>=7.12.0->tensortrade) (0.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.1.1->tensortrade) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.1.1->tensortrade) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.1.1->tensortrade) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.1.1->tensortrade) (4.32.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.1.1->tensortrade) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.1.1->tensortrade) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.1.1->tensortrade) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.25.0->tensortrade) (2022.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from plotly>=4.5.0->tensortrade) (1.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from plotly>=4.5.0->tensortrade) (8.0.1)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (2.6.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (3.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (0.37.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (0.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (3.20.0rc2)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (1.43.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (1.6.3)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (5.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.9/site-packages (from tensorflow>=2.1.0->tensortrade) (3.7.4.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.12.0->tensortrade) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.12.0->tensortrade) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.12.0->tensortrade) (0.2.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (1.8.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (2.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (3.3.6)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=7.12.0->tensortrade) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=7.12.0->tensortrade) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=7.12.0->tensortrade) (0.2.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (4.11.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.1.0->tensortrade) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#added by p-pl\n",
    "%pip install scipy tensortrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2041db20-3936-4da0-b753-12605a62187d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "bede9272-b231-4d56-97ef-e2e180cf0655",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "82aa9af4-5352-4e8b-dafa-83b334765f71",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas-ta==0.3.14b in /opt/conda/lib/python3.9/site-packages (0.3.14b0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from pandas-ta==0.3.14b) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->pandas-ta==0.3.14b) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->pandas-ta==0.3.14b) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.9/site-packages (from pandas->pandas-ta==0.3.14b) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->pandas-ta==0.3.14b) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gym==0.21.0 in /opt/conda/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from gym==0.21.0) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.9/site-packages (from gym==0.21.0) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.9/site-packages (7.7.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (6.13.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.2.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (62.1.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.29)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (2.15.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.9/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.11)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /opt/conda/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.3)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.13.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.9/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.11.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.9/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ray in /opt/conda/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from ray) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.9/site-packages (from ray) (3.20.0rc2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from ray) (2.27.1)\n",
      "Requirement already satisfied: virtualenv in /opt/conda/lib/python3.9/site-packages (from ray) (20.14.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from ray) (8.1.2)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.9/site-packages (from ray) (1.3.0)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.9/site-packages (from ray) (1.2.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /opt/conda/lib/python3.9/site-packages (from ray) (1.43.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.9/site-packages (from ray) (21.4.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from ray) (6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from ray) (1.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from ray) (3.6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from ray) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.9/site-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema->ray) (0.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->ray) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->ray) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->ray) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->ray) (1.26.9)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray) (2.5.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray) (0.3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ray[tune] in /opt/conda/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.0.3)\n",
      "Requirement already satisfied: virtualenv in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (20.14.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (6.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (3.20.0rc2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (2.27.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.3.0)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (3.6.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (8.1.2)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (21.4.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.43.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.19.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (4.4.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (2.5)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (0.8.9)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from ray[tune]) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.9/site-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema->ray[tune]) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->ray[tune]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->ray[tune]) (2022.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->ray[tune]) (2021.10.8)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray[tune]) (0.3.4)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray[tune]) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ray[rllib] in /opt/conda/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (4.4.0)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (3.6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (1.0.3)\n",
      "Requirement already satisfied: virtualenv in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (20.14.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (1.19.5)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (21.4.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (8.1.2)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (1.43.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (3.20.0rc2)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (1.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.4.3 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (3.5.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (1.8.0)\n",
      "Requirement already satisfied: gym<0.22 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (0.21.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (4.0.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (0.8.9)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (1.4.2)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (2.5)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (0.1.7)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.9/site-packages (from ray[rllib]) (0.19.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.9/site-packages (from grpcio<=1.43.0,>=1.28.1->ray[rllib]) (1.15.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from gym<0.22->ray[rllib]) (2.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (4.32.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (3.0.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (9.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib]) (21.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema->ray[rllib]) (0.18.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->ray[rllib]) (2022.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->ray[rllib]) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->ray[rllib]) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->ray[rllib]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->ray[rllib]) (3.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (2.8)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (2022.4.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (2.17.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray[rllib]) (0.3.4)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray[rllib]) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ray[serve] in /opt/conda/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (1.43.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (4.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (2.27.1)\n",
      "Requirement already satisfied: virtualenv in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (20.14.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (8.1.2)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (1.19.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (3.6.0)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (1.3.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (1.0.3)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (1.2.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (21.4.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (3.20.0rc2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (6.0)\n",
      "Requirement already satisfied: starlette in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.17.1)\n",
      "Requirement already satisfied: aiohttp>=3.7 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (3.8.1)\n",
      "Requirement already satisfied: gpustat>=1.0.0b1 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (1.0.0b1)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.3.11)\n",
      "Requirement already satisfied: prometheus-client<0.14.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.13.1)\n",
      "Requirement already satisfied: opencensus in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.9.0)\n",
      "Requirement already satisfied: aiohttp-cors in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.7.0)\n",
      "Requirement already satisfied: aiorwlock in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (1.3.0)\n",
      "Requirement already satisfied: uvicorn==0.16.0 in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.16.0)\n",
      "Requirement already satisfied: colorful in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.5.4)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (0.75.2)\n",
      "Requirement already satisfied: smart-open in /opt/conda/lib/python3.9/site-packages (from ray[serve]) (6.0.0)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.9/site-packages (from uvicorn==0.16.0->ray[serve]) (0.13.0)\n",
      "Requirement already satisfied: asgiref>=3.4.0 in /opt/conda/lib/python3.9/site-packages (from uvicorn==0.16.0->ray[serve]) (3.5.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]) (2.0.12)\n",
      "Requirement already satisfied: blessed>=1.17.1 in /opt/conda/lib/python3.9/site-packages (from gpustat>=1.0.0b1->ray[serve]) (1.19.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from gpustat>=1.0.0b1->ray[serve]) (5.9.0)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.9/site-packages (from gpustat>=1.0.0b1->ray[serve]) (1.15.0)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.9/site-packages (from gpustat>=1.0.0b1->ray[serve]) (7.352.0)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /opt/conda/lib/python3.9/site-packages (from fastapi->ray[serve]) (1.9.0)\n",
      "Requirement already satisfied: anyio<4,>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from starlette->ray[serve]) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema->ray[serve]) (0.18.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from opencensus->ray[serve]) (2.7.3)\n",
      "Requirement already satisfied: opencensus-context>=0.1.2 in /opt/conda/lib/python3.9/site-packages (from opencensus->ray[serve]) (0.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->ray[serve]) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->ray[serve]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->ray[serve]) (3.3)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray[serve]) (0.3.4)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /opt/conda/lib/python3.9/site-packages (from virtualenv->ray[serve]) (2.5.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.9/site-packages (from anyio<4,>=3.0.0->starlette->ray[serve]) (1.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.9/site-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[serve]) (0.2.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (1.56.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (1.35.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi->ray[serve]) (3.7.4.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (4.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (62.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ta in /opt/conda/lib/python3.9/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from ta) (1.19.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from ta) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->ta) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->ta) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: quantstats in /opt/conda/lib/python3.9/site-packages (0.0.56)\n",
      "Requirement already satisfied: tabulate>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from quantstats) (0.8.9)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from quantstats) (0.11.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from quantstats) (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /opt/conda/lib/python3.9/site-packages (from quantstats) (2.8.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.9/site-packages (from quantstats) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.9/site-packages (from quantstats) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from quantstats) (1.8.0)\n",
      "Requirement already satisfied: yfinance>=0.1.70 in /opt/conda/lib/python3.9/site-packages (from quantstats) (0.1.70)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->quantstats) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->quantstats) (4.32.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->quantstats) (9.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->quantstats) (3.0.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->quantstats) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->quantstats) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.24.0->quantstats) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.0->quantstats) (1.15.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/conda/lib/python3.9/site-packages (from yfinance>=0.1.70->quantstats) (0.0.10)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /opt/conda/lib/python3.9/site-packages (from yfinance>=0.1.70->quantstats) (4.8.0)\n",
      "Requirement already satisfied: requests>=2.26 in /opt/conda/lib/python3.9/site-packages (from yfinance>=0.1.70->quantstats) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26->yfinance>=0.1.70->quantstats) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26->yfinance>=0.1.70->quantstats) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26->yfinance>=0.1.70->quantstats) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.26->yfinance>=0.1.70->quantstats) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: feature_engine in /opt/conda/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.9/site-packages (from feature_engine) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.9/site-packages (from feature_engine) (1.8.0)\n",
      "Requirement already satisfied: pandas>=1.0.3 in /opt/conda/lib/python3.9/site-packages (from feature_engine) (1.4.2)\n",
      "Requirement already satisfied: statsmodels>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from feature_engine) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from feature_engine) (1.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.3->feature_engine) (2022.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.9/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.9/site-packages (from statsmodels>=0.11.1->feature_engine) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=21.3->statsmodels>=0.11.1->feature_engine) (3.0.8)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mplfinance in /opt/conda/lib/python3.9/site-packages (0.12.8b9)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from mplfinance) (3.5.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from mplfinance) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (4.32.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib->mplfinance) (9.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->mplfinance) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mplfinance) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.9/site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from optuna) (4.64.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from optuna) (1.4.35)\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.9/site-packages (from optuna) (1.7.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from optuna) (1.19.5)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.9/site-packages (from optuna) (6.6.0)\n",
      "Requirement already satisfied: cliff in /opt/conda/lib/python3.9/site-packages (from optuna) (3.10.1)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.9/site-packages (from optuna) (1.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.9/site-packages (from alembic->optuna) (1.2.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.9/site-packages (from cliff->optuna) (3.5.0)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from cliff->optuna) (2.4.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from cliff->optuna) (5.9.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.9/site-packages (from cliff->optuna) (3.3.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from cliff->optuna) (0.5.0)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.9/site-packages (from Mako->alembic->optuna) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas-ta==0.3.14b --pre\n",
    "%pip install gym==0.21.0\n",
    "%pip install ipywidgets\n",
    "%pip install ray\n",
    "%pip install -U \"ray[tune]\"\n",
    "%pip install -U \"ray[rllib]\"\n",
    "%pip install -U \"ray[serve]\"\n",
    "%pip install ta\n",
    "%pip install quantstats\n",
    "%pip install sklearn\n",
    "%pip install feature_engine\n",
    "%pip install --upgrade mplfinance\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c0bc4-b9bd-49e8-b79b-d11fc1aa8629",
   "metadata": {
    "id": "c23c0bc4-b9bd-49e8-b79b-d11fc1aa8629",
    "tags": []
   },
   "source": [
    "# Prepare and fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "022f3a95-c2d5-4737-906c-270ee437c03b",
   "metadata": {
    "id": "022f3a95-c2d5-4737-906c-270ee437c03b"
   },
   "outputs": [],
   "source": [
    "from tensortrade.data.cdd import CryptoDataDownload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "\n",
    "def prepare_data(df):\n",
    "    df['volume'] = np.int64(df['volume'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.sort_values(by='date', ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d %I:%M %p')\n",
    "    return df\n",
    "\n",
    "def fetch_data():\n",
    "    cdd = CryptoDataDownload()\n",
    "    bitfinex_data = cdd.fetch(\"Bitfinex\", \"USD\", \"BTC\", \"1h\")\n",
    "    bitfinex_data = bitfinex_data[['date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    bitfinex_data = prepare_data(bitfinex_data)\n",
    "    return bitfinex_data\n",
    "\n",
    "def load_csv(filename):\n",
    "    df = pd.read_csv('data/' + filename, skiprows=1)\n",
    "    #df.drop(columns=['symbol', 'volume_btc'], inplace=True)\n",
    "\n",
    "    # Fix timestamp from \"2019-10-17 09-AM\" to \"2019-10-17 09-00-00 AM\"\n",
    "    df['date'] = df['date'].str[:14] + '00-00 ' + df['date'].str[-2:]\n",
    "\n",
    "    return prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7711877c-da93-4743-acf3-bd03d23f07c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-15 06:00 AM</td>\n",
       "      <td>8723.800000</td>\n",
       "      <td>8793.000000</td>\n",
       "      <td>8714.9</td>\n",
       "      <td>8739.000000</td>\n",
       "      <td>8988053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-15 07:00 AM</td>\n",
       "      <td>8739.000000</td>\n",
       "      <td>8754.800000</td>\n",
       "      <td>8719.3</td>\n",
       "      <td>8743.000000</td>\n",
       "      <td>2288904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-15 08:00 AM</td>\n",
       "      <td>8743.000000</td>\n",
       "      <td>8743.100000</td>\n",
       "      <td>8653.2</td>\n",
       "      <td>8723.700000</td>\n",
       "      <td>8891773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-15 09:00 AM</td>\n",
       "      <td>8723.700000</td>\n",
       "      <td>8737.800000</td>\n",
       "      <td>8701.2</td>\n",
       "      <td>8708.100000</td>\n",
       "      <td>2054868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-15 10:00 AM</td>\n",
       "      <td>8708.100000</td>\n",
       "      <td>8855.700000</td>\n",
       "      <td>8695.8</td>\n",
       "      <td>8784.400000</td>\n",
       "      <td>17309722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34522</th>\n",
       "      <td>2022-04-24 09:00 PM</td>\n",
       "      <td>39526.977727</td>\n",
       "      <td>39670.000000</td>\n",
       "      <td>39505.0</td>\n",
       "      <td>39669.000000</td>\n",
       "      <td>6064813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34523</th>\n",
       "      <td>2022-04-24 10:00 PM</td>\n",
       "      <td>39669.000000</td>\n",
       "      <td>39798.000000</td>\n",
       "      <td>39566.0</td>\n",
       "      <td>39588.000000</td>\n",
       "      <td>2804385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34524</th>\n",
       "      <td>2022-04-24 11:00 PM</td>\n",
       "      <td>39588.000000</td>\n",
       "      <td>39593.585605</td>\n",
       "      <td>39461.0</td>\n",
       "      <td>39480.581239</td>\n",
       "      <td>2732269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34525</th>\n",
       "      <td>2022-04-25 12:00 AM</td>\n",
       "      <td>39473.000000</td>\n",
       "      <td>39526.000000</td>\n",
       "      <td>38706.0</td>\n",
       "      <td>38890.299273</td>\n",
       "      <td>15814016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34526</th>\n",
       "      <td>2022-04-25 01:00 AM</td>\n",
       "      <td>38891.000000</td>\n",
       "      <td>39023.000000</td>\n",
       "      <td>38792.0</td>\n",
       "      <td>38947.233442</td>\n",
       "      <td>3316250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34527 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date          open          high      low         close  \\\n",
       "0      2018-05-15 06:00 AM   8723.800000   8793.000000   8714.9   8739.000000   \n",
       "1      2018-05-15 07:00 AM   8739.000000   8754.800000   8719.3   8743.000000   \n",
       "2      2018-05-15 08:00 AM   8743.000000   8743.100000   8653.2   8723.700000   \n",
       "3      2018-05-15 09:00 AM   8723.700000   8737.800000   8701.2   8708.100000   \n",
       "4      2018-05-15 10:00 AM   8708.100000   8855.700000   8695.8   8784.400000   \n",
       "...                    ...           ...           ...      ...           ...   \n",
       "34522  2022-04-24 09:00 PM  39526.977727  39670.000000  39505.0  39669.000000   \n",
       "34523  2022-04-24 10:00 PM  39669.000000  39798.000000  39566.0  39588.000000   \n",
       "34524  2022-04-24 11:00 PM  39588.000000  39593.585605  39461.0  39480.581239   \n",
       "34525  2022-04-25 12:00 AM  39473.000000  39526.000000  38706.0  38890.299273   \n",
       "34526  2022-04-25 01:00 AM  38891.000000  39023.000000  38792.0  38947.233442   \n",
       "\n",
       "         volume  \n",
       "0       8988053  \n",
       "1       2288904  \n",
       "2       8891773  \n",
       "3       2054868  \n",
       "4      17309722  \n",
       "...         ...  \n",
       "34522   6064813  \n",
       "34523   2804385  \n",
       "34524   2732269  \n",
       "34525  15814016  \n",
       "34526   3316250  \n",
       "\n",
       "[34527 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdac31a-4ad2-4365-b061-bf0eaa5b4b49",
   "metadata": {},
   "source": [
    "## Create features for the feed module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efe71f2a-5779-4e49-9b1a-a2952f07052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import ta as ta1\n",
    "import pandas_ta as ta\n",
    "\n",
    "import quantstats as qs\n",
    "qs.extend_pandas()\n",
    "\n",
    "def fix_dataset_inconsistencies_without_backfilling(dataframe, fill_value=None):\n",
    "    dataframe = dataframe.replace([-np.inf, np.inf], np.nan)\n",
    "\n",
    "    return dataframe.fillna(axis='index', method='pad').dropna(axis='columns')\n",
    "\n",
    "def fix_dataset_inconsistencies(dataframe, fill_value=None):\n",
    "    dataframe = dataframe.replace([-np.inf, np.inf], np.nan)\n",
    "\n",
    "    #This is done to avoid filling middle holes with backfilling.\n",
    "    if fill_value is None:\n",
    "        dataframe.iloc[0,:] = \\\n",
    "            dataframe.apply(lambda column: column.iloc[column.first_valid_index()], axis='index')\n",
    "    else:\n",
    "        dataframe.iloc[0,:] = \\\n",
    "            dataframe.iloc[0,:].fillna(fill_value)\n",
    "\n",
    "    return dataframe.fillna(axis='index', method='pad').dropna(axis='columns')\n",
    "\n",
    "def rsi(price: 'pd.Series[pd.Float64Dtype]', period: float) -> 'pd.Series[pd.Float64Dtype]':\n",
    "    r = price.diff()\n",
    "    upside = np.minimum(r, 0).abs()\n",
    "    downside = np.maximum(r, 0).abs()\n",
    "    rs = upside.ewm(alpha=1 / period).mean() / downside.ewm(alpha=1 / period).mean()\n",
    "    return 100*(1 - (1 + rs) ** -1)\n",
    "\n",
    "def macd(price: 'pd.Series[pd.Float64Dtype]', fast: float, slow: float, signal: float) -> 'pd.Series[pd.Float64Dtype]':\n",
    "    fm = price.ewm(span=fast, adjust=False).mean()\n",
    "    sm = price.ewm(span=slow, adjust=False).mean()\n",
    "    md = fm - sm\n",
    "    signal = md - md.ewm(span=signal, adjust=False).mean()\n",
    "    return signal\n",
    "\n",
    "def generate_all_default_quantstats_features(data):\n",
    "    excluded_indicators = [\n",
    "        'compare',\n",
    "        'greeks',\n",
    "        'information_ratio',\n",
    "        'omega',\n",
    "        'r2',\n",
    "        'r_squared',\n",
    "        'rolling_greeks',\n",
    "        'warn',\n",
    "        'treynor_ratio'\n",
    "    ]\n",
    "    \n",
    "    indicators_list = [f for f in dir(qs.stats) if f[0] != '_' and f not in excluded_indicators]\n",
    "    \n",
    "    df = data.copy()\n",
    "    df = df.set_index('date')\n",
    "    df.index = pd.DatetimeIndex(df.index)\n",
    "\n",
    "    for indicator_name in indicators_list:\n",
    "        try:\n",
    "            print(indicator_name)\n",
    "            indicator = qs.stats.__dict__[indicator_name](df['close'])\n",
    "            if isinstance(indicator, pd.Series):\n",
    "                indicator = indicator.to_frame(name=indicator_name)\n",
    "                df = pd.concat([df, indicator], axis='columns')\n",
    "        except (pd.errors.InvalidIndexError, ValueError):\n",
    "            pass\n",
    "\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def generate_features(data):\n",
    "\n",
    "    # Add day of week as feature\n",
    "    data['date'] = pd.to_datetime(data[\"date\"])\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "    # Add hour of day as feature\n",
    "    data['hour_of_day'] = data['date'].dt.hour\n",
    "\n",
    "    # Generate all default indicators from ta library\n",
    "    ta1.add_all_ta_features(data, \n",
    "                            'open', \n",
    "                            'high', \n",
    "                            'low', \n",
    "                            'close', \n",
    "                            'volume', \n",
    "                            fillna=True)\n",
    "\n",
    "    # Naming convention across most technical indicator libraries\n",
    "    data = data.rename(columns={'open': 'Open', \n",
    "                                'high': 'High', \n",
    "                                'low': 'Low', \n",
    "                                'close': 'Close', \n",
    "                                'volume': 'Volume'})\n",
    "    data = data.set_index('date')\n",
    "\n",
    "    # Custom indicators\n",
    "    features = pd.DataFrame.from_dict({\n",
    "        'prev_open': data['Open'].shift(1),\n",
    "        'prev_high': data['High'].shift(1),\n",
    "        'prev_low': data['Low'].shift(1),\n",
    "        'prev_close': data['Close'].shift(1),\n",
    "        'prev_volume': data['Volume'].shift(1),\n",
    "        'vol_5': data['Close'].rolling(window=5).std().abs(),\n",
    "        'vol_10': data['Close'].rolling(window=10).std().abs(),\n",
    "        'vol_20': data['Close'].rolling(window=20).std().abs(),\n",
    "        'vol_30': data['Close'].rolling(window=30).std().abs(),\n",
    "        'vol_50': data['Close'].rolling(window=50).std().abs(),\n",
    "        'vol_60': data['Close'].rolling(window=60).std().abs(),\n",
    "        'vol_100': data['Close'].rolling(window=100).std().abs(),\n",
    "        'vol_200': data['Close'].rolling(window=200).std().abs(),\n",
    "        'ma_5': data['Close'].rolling(window=5).mean(),\n",
    "        'ma_10': data['Close'].rolling(window=10).mean(),\n",
    "        'ma_20': data['Close'].rolling(window=20).mean(),\n",
    "        'ma_30': data['Close'].rolling(window=30).mean(),\n",
    "        'ma_50': data['Close'].rolling(window=50).mean(),\n",
    "        'ma_60': data['Close'].rolling(window=60).mean(),\n",
    "        'ma_100': data['Close'].rolling(window=100).mean(),\n",
    "        'ma_200': data['Close'].rolling(window=200).mean(),\n",
    "        'ema_5': ta1.trend.ema_indicator(data['Close'], window=5, fillna=True),\n",
    "        'ema_9': ta1.trend.ema_indicator(data['Close'], window=9, fillna=True),\n",
    "        'ema_21': ta1.trend.ema_indicator(data['Close'], window=21, fillna=True),\n",
    "        'ema_60': ta1.trend.ema_indicator(data['Close'], window=60, fillna=True),\n",
    "        'ema_64': ta1.trend.ema_indicator(data['Close'], window=64, fillna=True),\n",
    "        'ema_120': ta1.trend.ema_indicator(data['Close'], window=120, fillna=True),\n",
    "        'lr_open': np.log(data['Open']).diff().fillna(0),\n",
    "        'lr_high': np.log(data['High']).diff().fillna(0),\n",
    "        'lr_low': np.log(data['Low']).diff().fillna(0),\n",
    "        'lr_close': np.log(data['Close']).diff().fillna(0),\n",
    "        'r_volume': data['Close'].diff().fillna(0),\n",
    "        'rsi_5': rsi(data['Close'], period=5),\n",
    "        'rsi_10': rsi(data['Close'], period=10),\n",
    "        'rsi_100': rsi(data['Close'], period=100),\n",
    "        'rsi_7': rsi(data['Close'], period=7),\n",
    "        'rsi_28': rsi(data['Close'], period=28),\n",
    "        'rsi_6': rsi(data['Close'], period=6),\n",
    "        'rsi_14': rsi(data['Close'], period=14),\n",
    "        'rsi_26': rsi(data['Close'], period=24),\n",
    "        'macd_normal': macd(data['Close'], fast=12, slow=26, signal=9),\n",
    "        'macd_short': macd(data['Close'], fast=10, slow=50, signal=5),\n",
    "        'macd_long': macd(data['Close'], fast=200, slow=100, signal=50),\n",
    "        'macd_wolfpack': macd(data['Close'], fast=3, slow=8, signal=9),\n",
    "    })\n",
    "\n",
    "    # Concatenate both manually and automatically generated features\n",
    "    data = pd.concat([data, features], axis='columns').fillna(method='pad')\n",
    "\n",
    "    # Remove potential column duplicates\n",
    "    data = data.loc[:,~data.columns.duplicated()]\n",
    "\n",
    "    # Revert naming convention\n",
    "    data = data.rename(columns={'Open': 'open', \n",
    "                                'High': 'high', \n",
    "                                'Low': 'low', \n",
    "                                'Close': 'close', \n",
    "                                'Volume': 'volume'})\n",
    "\n",
    "\n",
    "    data = data.reset_index()\n",
    "\n",
    "    # Generate all default quantstats features\n",
    "    df_quantstats = generate_all_default_quantstats_features(data)\n",
    "\n",
    "    # Concatenate both manually and automatically generated features\n",
    "    data = pd.concat([data, df_quantstats], axis='columns').fillna(method='pad')\n",
    "\n",
    "\n",
    "    # Remove potential column duplicates\n",
    "    data = data.loc[:,~data.columns.duplicated()]\n",
    "\n",
    "    # A lot of indicators generate NaNs at the beginning of DataFrames, so remove them\n",
    "    data = data.iloc[200:]\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    data = fix_dataset_inconsistencies_without_backfilling(data, fill_value=None)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70679eb9-03fc-4ef6-b81c-a64b6eb026db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/ta/trend.py:769: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "/opt/conda/lib/python3.9/site-packages/ta/trend.py:774: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n",
      "/opt/conda/lib/python3.9/site-packages/ta/trend.py:938: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  self._psar_up = pd.Series(index=self._psar.index)\n",
      "/opt/conda/lib/python3.9/site-packages/ta/trend.py:939: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  self._psar_down = pd.Series(index=self._psar.index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted_sortino\n",
      "autocorr_penalty\n",
      "avg_loss\n",
      "avg_return\n",
      "avg_win\n",
      "best\n",
      "cagr\n",
      "calmar\n",
      "common_sense_ratio\n",
      "comp\n",
      "compsum\n",
      "conditional_value_at_risk\n",
      "consecutive_losses\n",
      "consecutive_wins\n",
      "cpc_index\n",
      "cvar\n",
      "distribution\n",
      "drawdown_details\n",
      "expected_return\n",
      "expected_shortfall\n",
      "exposure\n",
      "gain_to_pain_ratio\n",
      "geometric_mean\n",
      "ghpr\n",
      "implied_volatility\n",
      "kelly_criterion\n",
      "kurtosis\n",
      "max_drawdown\n",
      "monthly_returns\n",
      "outlier_loss_ratio\n",
      "outlier_win_ratio\n",
      "outliers\n",
      "payoff_ratio\n",
      "pct_rank\n",
      "probabilistic_adjusted_sortino_ratio\n",
      "probabilistic_ratio\n",
      "probabilistic_sharpe_ratio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/quantstats/utils.py:68: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  return _pd.concat(dfs, 1, sort=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilistic_sortino_ratio\n",
      "profit_factor\n",
      "profit_ratio\n",
      "rar\n",
      "recovery_factor\n",
      "remove_outliers\n",
      "risk_of_ruin\n",
      "risk_return_ratio\n",
      "rolling_sharpe\n",
      "rolling_sortino\n",
      "rolling_volatility\n",
      "ror\n",
      "serenity_index\n",
      "sharpe\n",
      "skew\n",
      "smart_sharpe\n",
      "smart_sortino\n",
      "sortino\n",
      "tail_ratio\n",
      "to_drawdown_series\n",
      "ulcer_index\n",
      "ulcer_performance_index\n",
      "upi\n",
      "value_at_risk\n",
      "var\n",
      "volatility\n",
      "win_loss_ratio\n",
      "win_rate\n",
      "worst\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>volume_adi</th>\n",
       "      <th>volume_obv</th>\n",
       "      <th>...</th>\n",
       "      <th>rsi_26</th>\n",
       "      <th>macd_normal</th>\n",
       "      <th>macd_short</th>\n",
       "      <th>macd_long</th>\n",
       "      <th>macd_wolfpack</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>rolling_sharpe</th>\n",
       "      <th>rolling_sortino</th>\n",
       "      <th>rolling_volatility</th>\n",
       "      <th>to_drawdown_series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-23 14:00:00</td>\n",
       "      <td>7897.300000</td>\n",
       "      <td>7898.800000</td>\n",
       "      <td>7849.8</td>\n",
       "      <td>7877.400000</td>\n",
       "      <td>9341499</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.219515e+08</td>\n",
       "      <td>-153103304</td>\n",
       "      <td>...</td>\n",
       "      <td>65.542059</td>\n",
       "      <td>11.190548</td>\n",
       "      <td>10.871904</td>\n",
       "      <td>31.873058</td>\n",
       "      <td>19.596642</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-0.811487</td>\n",
       "      <td>-1.144302</td>\n",
       "      <td>0.072620</td>\n",
       "      <td>-0.103251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-23 15:00:00</td>\n",
       "      <td>7877.400000</td>\n",
       "      <td>7889.700000</td>\n",
       "      <td>7661.0</td>\n",
       "      <td>7700.000000</td>\n",
       "      <td>23679375</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>-1.375548e+08</td>\n",
       "      <td>-176782679</td>\n",
       "      <td>...</td>\n",
       "      <td>72.698849</td>\n",
       "      <td>1.333779</td>\n",
       "      <td>-5.426751</td>\n",
       "      <td>34.355233</td>\n",
       "      <td>-24.639480</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-1.248391</td>\n",
       "      <td>-1.633909</td>\n",
       "      <td>0.079103</td>\n",
       "      <td>-0.123446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-23 16:00:00</td>\n",
       "      <td>7700.000000</td>\n",
       "      <td>7700.100000</td>\n",
       "      <td>7548.1</td>\n",
       "      <td>7605.400000</td>\n",
       "      <td>42144843</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>-1.479246e+08</td>\n",
       "      <td>-218927522</td>\n",
       "      <td>...</td>\n",
       "      <td>75.527202</td>\n",
       "      <td>-10.060459</td>\n",
       "      <td>-21.497215</td>\n",
       "      <td>37.504922</td>\n",
       "      <td>-51.837145</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-1.612964</td>\n",
       "      <td>-2.069373</td>\n",
       "      <td>0.080681</td>\n",
       "      <td>-0.134215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-23 17:00:00</td>\n",
       "      <td>7605.400000</td>\n",
       "      <td>7623.600000</td>\n",
       "      <td>7441.8</td>\n",
       "      <td>7511.100000</td>\n",
       "      <td>38711817</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>-1.571235e+08</td>\n",
       "      <td>-257639339</td>\n",
       "      <td>...</td>\n",
       "      <td>77.907846</td>\n",
       "      <td>-21.778972</td>\n",
       "      <td>-36.146245</td>\n",
       "      <td>41.269618</td>\n",
       "      <td>-66.773623</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-1.797159</td>\n",
       "      <td>-2.272346</td>\n",
       "      <td>0.082309</td>\n",
       "      <td>-0.144950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-23 18:00:00</td>\n",
       "      <td>7511.100000</td>\n",
       "      <td>7551.600000</td>\n",
       "      <td>7403.0</td>\n",
       "      <td>7489.100000</td>\n",
       "      <td>23046091</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.534634e+08</td>\n",
       "      <td>-280685430</td>\n",
       "      <td>...</td>\n",
       "      <td>78.418914</td>\n",
       "      <td>-28.422775</td>\n",
       "      <td>-41.976877</td>\n",
       "      <td>44.917996</td>\n",
       "      <td>-57.191729</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-1.879146</td>\n",
       "      <td>-2.372706</td>\n",
       "      <td>0.082361</td>\n",
       "      <td>-0.147455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34322</th>\n",
       "      <td>2022-04-24 21:00:00</td>\n",
       "      <td>39526.977727</td>\n",
       "      <td>39670.000000</td>\n",
       "      <td>39505.0</td>\n",
       "      <td>39669.000000</td>\n",
       "      <td>6064813</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>1.026967e+10</td>\n",
       "      <td>-4100671313</td>\n",
       "      <td>...</td>\n",
       "      <td>52.634193</td>\n",
       "      <td>1.342651</td>\n",
       "      <td>10.198366</td>\n",
       "      <td>58.527379</td>\n",
       "      <td>16.254117</td>\n",
       "      <td>51.666667</td>\n",
       "      <td>-1.339597</td>\n",
       "      <td>-1.636928</td>\n",
       "      <td>0.065078</td>\n",
       "      <td>-0.421743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34323</th>\n",
       "      <td>2022-04-24 22:00:00</td>\n",
       "      <td>39669.000000</td>\n",
       "      <td>39798.000000</td>\n",
       "      <td>39566.0</td>\n",
       "      <td>39588.000000</td>\n",
       "      <td>2804385</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1.026740e+10</td>\n",
       "      <td>-4103475698</td>\n",
       "      <td>...</td>\n",
       "      <td>54.261644</td>\n",
       "      <td>-0.063390</td>\n",
       "      <td>7.009476</td>\n",
       "      <td>57.143399</td>\n",
       "      <td>3.113579</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>-1.322784</td>\n",
       "      <td>-1.617026</td>\n",
       "      <td>0.065042</td>\n",
       "      <td>-0.422924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34324</th>\n",
       "      <td>2022-04-24 23:00:00</td>\n",
       "      <td>39588.000000</td>\n",
       "      <td>39593.585605</td>\n",
       "      <td>39461.0</td>\n",
       "      <td>39480.581239</td>\n",
       "      <td>2732269</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>1.026548e+10</td>\n",
       "      <td>-4106207967</td>\n",
       "      <td>...</td>\n",
       "      <td>56.337651</td>\n",
       "      <td>-7.461239</td>\n",
       "      <td>-4.541306</td>\n",
       "      <td>56.723628</td>\n",
       "      <td>-24.269541</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-1.417697</td>\n",
       "      <td>-1.730603</td>\n",
       "      <td>0.065119</td>\n",
       "      <td>-0.424490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34325</th>\n",
       "      <td>2022-04-25 00:00:00</td>\n",
       "      <td>39473.000000</td>\n",
       "      <td>39526.000000</td>\n",
       "      <td>38706.0</td>\n",
       "      <td>38890.299273</td>\n",
       "      <td>15814016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.025677e+10</td>\n",
       "      <td>-4122021983</td>\n",
       "      <td>...</td>\n",
       "      <td>65.354551</td>\n",
       "      <td>-49.025026</td>\n",
       "      <td>-65.489970</td>\n",
       "      <td>61.773491</td>\n",
       "      <td>-157.066417</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-1.818277</td>\n",
       "      <td>-2.163477</td>\n",
       "      <td>0.068277</td>\n",
       "      <td>-0.433094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34326</th>\n",
       "      <td>2022-04-25 01:00:00</td>\n",
       "      <td>38891.000000</td>\n",
       "      <td>39023.000000</td>\n",
       "      <td>38792.0</td>\n",
       "      <td>38947.233442</td>\n",
       "      <td>3316250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.025791e+10</td>\n",
       "      <td>-4118705733</td>\n",
       "      <td>...</td>\n",
       "      <td>64.023825</td>\n",
       "      <td>-68.408959</td>\n",
       "      <td>-86.005456</td>\n",
       "      <td>65.789737</td>\n",
       "      <td>-147.307615</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>-1.791796</td>\n",
       "      <td>-2.133185</td>\n",
       "      <td>0.068316</td>\n",
       "      <td>-0.432264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34327 rows  142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date          open          high      low         close  \\\n",
       "0     2018-05-23 14:00:00   7897.300000   7898.800000   7849.8   7877.400000   \n",
       "1     2018-05-23 15:00:00   7877.400000   7889.700000   7661.0   7700.000000   \n",
       "2     2018-05-23 16:00:00   7700.000000   7700.100000   7548.1   7605.400000   \n",
       "3     2018-05-23 17:00:00   7605.400000   7623.600000   7441.8   7511.100000   \n",
       "4     2018-05-23 18:00:00   7511.100000   7551.600000   7403.0   7489.100000   \n",
       "...                   ...           ...           ...      ...           ...   \n",
       "34322 2022-04-24 21:00:00  39526.977727  39670.000000  39505.0  39669.000000   \n",
       "34323 2022-04-24 22:00:00  39669.000000  39798.000000  39566.0  39588.000000   \n",
       "34324 2022-04-24 23:00:00  39588.000000  39593.585605  39461.0  39480.581239   \n",
       "34325 2022-04-25 00:00:00  39473.000000  39526.000000  38706.0  38890.299273   \n",
       "34326 2022-04-25 01:00:00  38891.000000  39023.000000  38792.0  38947.233442   \n",
       "\n",
       "         volume  day_of_week  hour_of_day    volume_adi  volume_obv  ...  \\\n",
       "0       9341499            2           14 -1.219515e+08  -153103304  ...   \n",
       "1      23679375            2           15 -1.375548e+08  -176782679  ...   \n",
       "2      42144843            2           16 -1.479246e+08  -218927522  ...   \n",
       "3      38711817            2           17 -1.571235e+08  -257639339  ...   \n",
       "4      23046091            2           18 -1.534634e+08  -280685430  ...   \n",
       "...         ...          ...          ...           ...         ...  ...   \n",
       "34322   6064813            6           21  1.026967e+10 -4100671313  ...   \n",
       "34323   2804385            6           22  1.026740e+10 -4103475698  ...   \n",
       "34324   2732269            6           23  1.026548e+10 -4106207967  ...   \n",
       "34325  15814016            0            0  1.025677e+10 -4122021983  ...   \n",
       "34326   3316250            0            1  1.025791e+10 -4118705733  ...   \n",
       "\n",
       "          rsi_26  macd_normal  macd_short  macd_long  macd_wolfpack  \\\n",
       "0      65.542059    11.190548   10.871904  31.873058      19.596642   \n",
       "1      72.698849     1.333779   -5.426751  34.355233     -24.639480   \n",
       "2      75.527202   -10.060459  -21.497215  37.504922     -51.837145   \n",
       "3      77.907846   -21.778972  -36.146245  41.269618     -66.773623   \n",
       "4      78.418914   -28.422775  -41.976877  44.917996     -57.191729   \n",
       "...          ...          ...         ...        ...            ...   \n",
       "34322  52.634193     1.342651   10.198366  58.527379      16.254117   \n",
       "34323  54.261644    -0.063390    7.009476  57.143399       3.113579   \n",
       "34324  56.337651    -7.461239   -4.541306  56.723628     -24.269541   \n",
       "34325  65.354551   -49.025026  -65.489970  61.773491    -157.066417   \n",
       "34326  64.023825   -68.408959  -86.005456  65.789737    -147.307615   \n",
       "\n",
       "        pct_rank  rolling_sharpe  rolling_sortino  rolling_volatility  \\\n",
       "0      10.000000       -0.811487        -1.144302            0.072620   \n",
       "1       1.666667       -1.248391        -1.633909            0.079103   \n",
       "2       1.666667       -1.612964        -2.069373            0.080681   \n",
       "3       1.666667       -1.797159        -2.272346            0.082309   \n",
       "4       1.666667       -1.879146        -2.372706            0.082361   \n",
       "...          ...             ...              ...                 ...   \n",
       "34322  51.666667       -1.339597        -1.636928            0.065078   \n",
       "34323  26.666667       -1.322784        -1.617026            0.065042   \n",
       "34324  10.000000       -1.417697        -1.730603            0.065119   \n",
       "34325   1.666667       -1.818277        -2.163477            0.068277   \n",
       "34326   3.333333       -1.791796        -2.133185            0.068316   \n",
       "\n",
       "       to_drawdown_series  \n",
       "0               -0.103251  \n",
       "1               -0.123446  \n",
       "2               -0.134215  \n",
       "3               -0.144950  \n",
       "4               -0.147455  \n",
       "...                   ...  \n",
       "34322           -0.421743  \n",
       "34323           -0.422924  \n",
       "34324           -0.424490  \n",
       "34325           -0.433094  \n",
       "34326           -0.432264  \n",
       "\n",
       "[34327 rows x 142 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = generate_features(data)\n",
    "# remove not needed features\n",
    "to_drop = ['others_dlr', 'compsum']\n",
    "data = data.drop(columns=to_drop)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ec36a-138e-4e5a-b475-b060acabd207",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Remove features with low variance before splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "501c9dd8-4188-44e5-852d-2f0f0fb4a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34327, 142)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "date = data[['date']].copy()\n",
    "data = data.drop(columns=['date'])\n",
    "sel.fit(data)\n",
    "data[data.columns[sel.get_support(indices=True)]]\n",
    "data = pd.concat([date, data], axis='columns')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdf738-453f-456e-9827-926f26ca96da",
   "metadata": {
    "id": "f6ef6f85-8116-4be7-821d-6f5b897ade5b",
    "tags": []
   },
   "source": [
    "# Setup which data to use for training and which data to use for evaluation of RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22d15ac6-4546-4a5d-bf7d-c117cb231f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data):\n",
    "    X = data.copy()\n",
    "    y = X['close'].pct_change()\n",
    "\n",
    "    X_train_test, X_valid, y_train_test, y_valid = \\\n",
    "        train_test_split(data, data['close'].pct_change(), train_size=0.67, test_size=0.33, shuffle=False)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train_test, y_train_test, train_size=0.50, test_size=0.50, shuffle=False)\n",
    "\n",
    "    return X_train, X_test, X_valid, y_train, y_test, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18402c18-25ef-48ea-85e3-0d7178971e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid, y_train, y_test, y_valid = \\\n",
    "    split_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc077fdd-4098-4121-a93b-5dda4d2750c5",
   "metadata": {},
   "source": [
    "# precalculate_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "351ebdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008422818026206175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "34322    0\n",
       "34323    0\n",
       "34324    0\n",
       "34325    0\n",
       "34326    0\n",
       "Name: close, Length: 34327, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def estimate_outliers(data):\n",
    "    return iqr(data) * 1.5\n",
    "\n",
    "def estimate_percent_gains(data, column='close'):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns)\n",
    "    return gains\n",
    "\n",
    "def get_returns(data, column='close'):\n",
    "    return fix_dataset_inconsistencies(data[[column]].pct_change(), fill_value=0)\n",
    "\n",
    "def precalculate_ground_truths(data, column='close', threshold=None):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns) if threshold is None else threshold\n",
    "    print(gains)\n",
    "    binary_gains = (returns[column] > gains).astype(int)\n",
    "    return binary_gains\n",
    "\n",
    "def is_null(data):\n",
    "    return data.isnull().sum().sum() > 0\n",
    "\n",
    "precalculate_ground_truths(data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b47f8984-cc97-450b-ba94-59647a4aabb6",
   "metadata": {},
   "source": [
    "expected output:\n",
    "\n",
    "0.008422818026206175\n",
    "0        0\n",
    "1        0\n",
    "2        0\n",
    "3        0\n",
    "4        0\n",
    "        ..\n",
    "34322    0\n",
    "34323    0\n",
    "34324    0\n",
    "34325    0\n",
    "34326    0\n",
    "Name: close, Length: 34327, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dddb3e-cc23-40a2-bdb5-a5da6b735f62",
   "metadata": {},
   "source": [
    "## Implement basic feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94b3f1ae-dbb1-4672-b034-fe454ff8874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=9311)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9350)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9379)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9255)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9405)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9392)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9476)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9285)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9557)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9366)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9434)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9268)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9571)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9518)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9298)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9450)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9531)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9544)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9324)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9349)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9489)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9463)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9505)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=9418)\u001b[0m I have finished the episode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectBySingleFeaturePerformance(cv=5,\n",
       "                                 estimator=RandomForestClassifier(n_jobs=-1,\n",
       "                                                                  random_state=1990),\n",
       "                                 threshold=0.65)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from feature_engine.selection import SelectBySingleFeaturePerformance\n",
    "\n",
    "def estimate_outliers(data):\n",
    "    return iqr(data) * 1.5\n",
    "\n",
    "def estimate_percent_gains(data, column='close'):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns)\n",
    "    return gains\n",
    "\n",
    "def get_returns(data, column='close'):\n",
    "    return fix_dataset_inconsistencies(data[[column]].pct_change(), fill_value=0)\n",
    "\n",
    "def precalculate_ground_truths(data, column='close', threshold=None):\n",
    "    returns = get_returns(data, column=column)\n",
    "    gains = estimate_outliers(returns) if threshold is None else threshold\n",
    "    binary_gains = (returns[column] > gains).astype(int)\n",
    "    return binary_gains\n",
    "\n",
    "def is_null(data):\n",
    "    return data.isnull().sum().sum() > 0\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "                            random_state=1990, \n",
    "                            n_jobs=-1)\n",
    "\n",
    "sel = SelectBySingleFeaturePerformance(variables=None, \n",
    "                                       estimator=rf, \n",
    "                                       scoring=\"roc_auc\", \n",
    "                                       cv=5, \n",
    "                                       threshold=0.65)\n",
    "\n",
    "sel.fit(X_train, precalculate_ground_truths(X_train, column='close'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f30be043-eca1-41ac-ab2b-713ff832ed2b",
   "metadata": {},
   "source": [
    "expected output:\n",
    "\n",
    "SelectBySingleFeaturePerformance(cv=5,\n",
    "                                 estimator=RandomForestClassifier(n_jobs=-1,\n",
    "                                                                  random_state=1990),\n",
    "                                 threshold=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73845b81-1151-4aa0-b372-4901188d9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "feature_performance = pd.Series(sel.feature_performance_).sort_values(ascending=False)\n",
    "# feature_performance.plot.bar(figsize=(40, 10))\n",
    "# plt.title('Performance of ML models trained with individual features')\n",
    "# plt.ylabel('roc-auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "228b820b-f0bf-4288-84e5-6db66c7c8647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open',\n",
       " 'high',\n",
       " 'low',\n",
       " 'close',\n",
       " 'day_of_week',\n",
       " 'hour_of_day',\n",
       " 'volume_adi',\n",
       " 'volume_obv',\n",
       " 'volume_cmf',\n",
       " 'volume_fi',\n",
       " 'volume_sma_em',\n",
       " 'volume_vwap',\n",
       " 'volume_mfi',\n",
       " 'volume_nvi',\n",
       " 'volatility_bbm',\n",
       " 'volatility_bbh',\n",
       " 'volatility_bbl',\n",
       " 'volatility_bbw',\n",
       " 'volatility_bbp',\n",
       " 'volatility_bbhi',\n",
       " 'volatility_bbli',\n",
       " 'volatility_kcc',\n",
       " 'volatility_kch',\n",
       " 'volatility_kcl',\n",
       " 'volatility_kcw',\n",
       " 'volatility_kcp',\n",
       " 'volatility_kcli',\n",
       " 'volatility_dcl',\n",
       " 'volatility_dch',\n",
       " 'volatility_dcm',\n",
       " 'volatility_dcw',\n",
       " 'volatility_dcp',\n",
       " 'volatility_atr',\n",
       " 'volatility_ui',\n",
       " 'trend_macd',\n",
       " 'trend_macd_signal',\n",
       " 'trend_macd_diff',\n",
       " 'trend_sma_fast',\n",
       " 'trend_sma_slow',\n",
       " 'trend_ema_fast',\n",
       " 'trend_ema_slow',\n",
       " 'trend_vortex_ind_pos',\n",
       " 'trend_vortex_ind_neg',\n",
       " 'trend_vortex_ind_diff',\n",
       " 'trend_trix',\n",
       " 'trend_mass_index',\n",
       " 'trend_dpo',\n",
       " 'trend_kst',\n",
       " 'trend_kst_sig',\n",
       " 'trend_kst_diff',\n",
       " 'trend_ichimoku_conv',\n",
       " 'trend_ichimoku_base',\n",
       " 'trend_ichimoku_a',\n",
       " 'trend_ichimoku_b',\n",
       " 'trend_stc',\n",
       " 'trend_adx',\n",
       " 'trend_adx_pos',\n",
       " 'trend_adx_neg',\n",
       " 'trend_cci',\n",
       " 'trend_visual_ichimoku_a',\n",
       " 'trend_visual_ichimoku_b',\n",
       " 'trend_aroon_down',\n",
       " 'trend_aroon_ind',\n",
       " 'trend_psar_up',\n",
       " 'trend_psar_down',\n",
       " 'trend_psar_up_indicator',\n",
       " 'trend_psar_down_indicator',\n",
       " 'momentum_rsi',\n",
       " 'momentum_stoch_rsi_k',\n",
       " 'momentum_stoch_rsi_d',\n",
       " 'momentum_tsi',\n",
       " 'momentum_uo',\n",
       " 'momentum_stoch',\n",
       " 'momentum_stoch_signal',\n",
       " 'momentum_wr',\n",
       " 'momentum_ao',\n",
       " 'momentum_roc',\n",
       " 'momentum_ppo',\n",
       " 'momentum_ppo_signal',\n",
       " 'momentum_ppo_hist',\n",
       " 'momentum_pvo',\n",
       " 'momentum_pvo_signal',\n",
       " 'momentum_pvo_hist',\n",
       " 'momentum_kama',\n",
       " 'others_cr',\n",
       " 'prev_open',\n",
       " 'prev_high',\n",
       " 'prev_low',\n",
       " 'prev_close',\n",
       " 'prev_volume',\n",
       " 'vol_5',\n",
       " 'vol_10',\n",
       " 'vol_20',\n",
       " 'vol_30',\n",
       " 'vol_50',\n",
       " 'vol_60',\n",
       " 'vol_100',\n",
       " 'vol_200',\n",
       " 'ma_5',\n",
       " 'ma_10',\n",
       " 'ma_20',\n",
       " 'ma_30',\n",
       " 'ma_50',\n",
       " 'ma_60',\n",
       " 'ma_100',\n",
       " 'ma_200',\n",
       " 'ema_5',\n",
       " 'ema_9',\n",
       " 'ema_21',\n",
       " 'ema_60',\n",
       " 'ema_64',\n",
       " 'ema_120',\n",
       " 'lr_open',\n",
       " 'lr_low',\n",
       " 'rsi_5',\n",
       " 'rsi_10',\n",
       " 'rsi_100',\n",
       " 'rsi_7',\n",
       " 'rsi_28',\n",
       " 'rsi_6',\n",
       " 'rsi_14',\n",
       " 'rsi_26',\n",
       " 'macd_normal',\n",
       " 'macd_short',\n",
       " 'macd_long',\n",
       " 'pct_rank',\n",
       " 'rolling_sharpe',\n",
       " 'rolling_sortino',\n",
       " 'rolling_volatility',\n",
       " 'to_drawdown_series']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_drop = sel.features_to_drop_\n",
    "# Keep some values no matter what\n",
    "to_drop = list(set(features_to_drop) - set(['open', 'high', 'low', 'close', 'volume', 'day_of_week', 'hour_of_day']))\n",
    "len(to_drop)\n",
    "features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49a0a65e-b758-470d-a06e-35fd085b188c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11499, 18), (11500, 18), (11328, 18))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)\n",
    "X_valid = X_valid.drop(columns=to_drop)\n",
    "\n",
    "X_train.shape, X_test.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "771b5a3d-4025-4206-bd60-7e992b63fc75",
   "metadata": {},
   "source": [
    "expected output:\n",
    "\n",
    "((11499, 18), (11500, 18), (11328, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc593487-0e67-4940-9d06-8f84064470a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'open',\n",
       " 'high',\n",
       " 'low',\n",
       " 'close',\n",
       " 'volume',\n",
       " 'day_of_week',\n",
       " 'hour_of_day',\n",
       " 'volume_em',\n",
       " 'volume_vpt',\n",
       " 'volatility_kchi',\n",
       " 'trend_aroon_up',\n",
       " 'momentum_stoch_rsi',\n",
       " 'others_dr',\n",
       " 'lr_high',\n",
       " 'lr_close',\n",
       " 'r_volume',\n",
       " 'macd_wolfpack']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be34f49e-4c1c-4ab3-b573-987ce812d3f7",
   "metadata": {},
   "source": [
    "expected output:\n",
    "\n",
    "['date',\n",
    " 'open',\n",
    " 'high',\n",
    " 'low',\n",
    " 'close',\n",
    " 'volume',\n",
    " 'day_of_week',\n",
    " 'hour_of_day',\n",
    " 'volume_em',\n",
    " 'volume_vpt',\n",
    " 'volatility_kchi',\n",
    " 'trend_aroon_up',\n",
    " 'momentum_stoch_rsi',\n",
    " 'others_dr',\n",
    " 'lr_high',\n",
    " 'lr_close',\n",
    " 'r_volume',\n",
    " 'macd_wolfpack']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bfb53-d874-4dc9-9d29-abf27d42cff8",
   "metadata": {},
   "source": [
    "## Normalize the dataset subsets to make the model converge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdef363e-b5ae-49f6-9acf-6a553fdbbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "scaler_type = MinMaxScaler\n",
    "\n",
    "def get_feature_scalers(X, scaler_type=scaler_type):\n",
    "    scalers = []\n",
    "    for name in list(X.columns[X.columns != 'date']):\n",
    "        scalers.append(scaler_type().fit(X[name].values.reshape(-1, 1)))\n",
    "    return scalers\n",
    "\n",
    "def get_scaler_transforms(X, scalers):\n",
    "    X_scaled = []\n",
    "    for name, scaler in zip(list(X.columns[X.columns != 'date']), scalers):\n",
    "        X_scaled.append(scaler.transform(X[name].values.reshape(-1, 1)))\n",
    "    X_scaled = pd.concat([pd.DataFrame(column, columns=[name]) for name, column in \\\n",
    "                          zip(list(X.columns[X.columns != 'date']), X_scaled)], axis='columns')\n",
    "    return X_scaled\n",
    "\n",
    "def scale_numpy_array(np_arr, scaler_type = scaler_type):\n",
    "    return scaler_type().fit_transform(np_arr, (-1,1))\n",
    "\n",
    "def normalize_data(X_train, X_test, X_valid):\n",
    "    X_train_test = pd.concat([X_train, X_test], axis='index')\n",
    "    X_train_test_valid = pd.concat([X_train_test, X_valid], axis='index')\n",
    "\n",
    "    X_train_test_dates = X_train_test[['date']]\n",
    "    X_train_test_valid_dates = X_train_test_valid[['date']]\n",
    "\n",
    "    X_train_test = X_train_test.drop(columns=['date'])\n",
    "    X_train_test_valid = X_train_test_valid.drop(columns=['date'])\n",
    "\n",
    "    train_test_scalers = \\\n",
    "        get_feature_scalers(X_train_test, \n",
    "                            scaler_type=scaler_type)\n",
    "    train_test_valid_scalers = \\\n",
    "        get_feature_scalers(X_train_test_valid, \n",
    "                            scaler_type=scaler_type)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        get_scaler_transforms(X_train_test, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_scalers)\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        get_scaler_transforms(X_train_test_valid, \n",
    "                              train_test_valid_scalers)\n",
    "\n",
    "    X_train_test_scaled = \\\n",
    "        pd.concat([X_train_test_dates, \n",
    "                   X_train_test_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled], \n",
    "                  axis='columns')\n",
    "    X_train_test_valid_scaled_leaking = \\\n",
    "        pd.concat([X_train_test_valid_dates, \n",
    "                   X_train_test_valid_scaled_leaking], \n",
    "                  axis='columns')\n",
    "\n",
    "    X_train_scaled = X_train_test_scaled.iloc[:X_train.shape[0]]\n",
    "    X_test_scaled = X_train_test_scaled.iloc[X_train.shape[0]:]\n",
    "    X_valid_scaled = X_train_test_valid_scaled.iloc[X_train_test.shape[0]:]\n",
    "    X_valid_scaled_leaking = X_train_test_valid_scaled_leaking.iloc[X_train_test.shape[0]:]\n",
    "\n",
    "    return (train_test_scalers, \n",
    "            train_test_valid_scalers, \n",
    "            X_train_scaled, \n",
    "            X_test_scaled, \n",
    "            X_valid_scaled, \n",
    "            X_valid_scaled_leaking)\n",
    "\n",
    "train_test_scalers, train_test_valid_scalers, X_train_scaled, X_test_scaled, X_valid_scaled, X_valid_scaled_leaking = \\\n",
    "    normalize_data(X_train, X_test, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aee2204c-640c-4f7e-b082-1392f48c585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "train_csv = os.path.join(cwd, 'train.csv')\n",
    "test_csv = os.path.join(cwd, 'test.csv')\n",
    "valid_csv = os.path.join(cwd, 'valid.csv')\n",
    "train_scaled_csv = os.path.join(cwd, 'train_scaled.csv')\n",
    "test_scaled_csv = os.path.join(cwd, 'test_scaled.csv')\n",
    "valid_scaled_csv = os.path.join(cwd, 'valid_scaled.csv')\n",
    "valid_scaled_leaking_csv = os.path.join(cwd, 'valid_scaled_leaking.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b52cc2",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0445d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(train_csv, index=False)\n",
    "X_test.to_csv(test_csv, index=False)\n",
    "X_valid.to_csv(valid_csv, index=False)\n",
    "X_train_scaled.to_csv(train_scaled_csv, index=False)\n",
    "X_test_scaled.to_csv(test_scaled_csv, index=False)\n",
    "X_valid_scaled.to_csv(valid_scaled_csv, index=False)\n",
    "X_valid_scaled_leaking.to_csv(valid_scaled_leaking_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d22ef",
   "metadata": {},
   "source": [
    "### Load from CSV if data previously saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e43009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv(train_csv)\n",
    "X_test = pd.read_csv(test_csv)\n",
    "X_valid = pd.read_csv(valid_csv)\n",
    "X_train_scaled = pd.read_csv(train_scaled_csv)\n",
    "X_test_scaled = pd.read_csv(test_scaled_csv)\n",
    "X_valid_scaled = pd.read_csv(valid_scaled_csv)\n",
    "X_valid_scaled_leaking = pd.read_csv(valid_scaled_leaking_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56e34a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>volume_em</th>\n",
       "      <th>volume_vpt</th>\n",
       "      <th>volatility_kchi</th>\n",
       "      <th>trend_aroon_up</th>\n",
       "      <th>momentum_stoch_rsi</th>\n",
       "      <th>others_dr</th>\n",
       "      <th>lr_high</th>\n",
       "      <th>lr_close</th>\n",
       "      <th>r_volume</th>\n",
       "      <th>macd_wolfpack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>2021-01-05 16:00:00</td>\n",
       "      <td>0.923977</td>\n",
       "      <td>0.932989</td>\n",
       "      <td>0.929314</td>\n",
       "      <td>0.936207</td>\n",
       "      <td>0.070038</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.545227</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495664</td>\n",
       "      <td>0.513635</td>\n",
       "      <td>0.541819</td>\n",
       "      <td>0.812224</td>\n",
       "      <td>0.784848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11496</th>\n",
       "      <td>2021-01-05 17:00:00</td>\n",
       "      <td>0.936442</td>\n",
       "      <td>0.936033</td>\n",
       "      <td>0.945385</td>\n",
       "      <td>0.944905</td>\n",
       "      <td>0.038781</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>0.548449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.487706</td>\n",
       "      <td>0.486908</td>\n",
       "      <td>0.533926</td>\n",
       "      <td>0.782758</td>\n",
       "      <td>0.797967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>2021-01-05 18:00:00</td>\n",
       "      <td>0.944301</td>\n",
       "      <td>0.959938</td>\n",
       "      <td>0.956678</td>\n",
       "      <td>0.966649</td>\n",
       "      <td>0.257846</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.571942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.521020</td>\n",
       "      <td>0.559102</td>\n",
       "      <td>0.566811</td>\n",
       "      <td>0.913431</td>\n",
       "      <td>0.900446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>2021-01-05 19:00:00</td>\n",
       "      <td>0.965903</td>\n",
       "      <td>0.981439</td>\n",
       "      <td>0.976098</td>\n",
       "      <td>0.978224</td>\n",
       "      <td>0.276735</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.585708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494305</td>\n",
       "      <td>0.549192</td>\n",
       "      <td>0.540472</td>\n",
       "      <td>0.811583</td>\n",
       "      <td>0.925196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>2021-01-05 20:00:00</td>\n",
       "      <td>0.977343</td>\n",
       "      <td>0.977771</td>\n",
       "      <td>0.982894</td>\n",
       "      <td>0.986026</td>\n",
       "      <td>0.092179</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.561554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.484602</td>\n",
       "      <td>0.463870</td>\n",
       "      <td>0.530843</td>\n",
       "      <td>0.773790</td>\n",
       "      <td>0.884411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date      open      high       low     close    volume  \\\n",
       "11495  2021-01-05 16:00:00  0.923977  0.932989  0.929314  0.936207  0.070038   \n",
       "11496  2021-01-05 17:00:00  0.936442  0.936033  0.945385  0.944905  0.038781   \n",
       "11497  2021-01-05 18:00:00  0.944301  0.959938  0.956678  0.966649  0.257846   \n",
       "11498  2021-01-05 19:00:00  0.965903  0.981439  0.976098  0.978224  0.276735   \n",
       "11499  2021-01-05 20:00:00  0.977343  0.977771  0.982894  0.986026  0.092179   \n",
       "\n",
       "       day_of_week  hour_of_day  volume_em  volume_vpt  volatility_kchi  \\\n",
       "11495     0.166667     0.695652   0.003030    0.545227              1.0   \n",
       "11496     0.166667     0.739130   0.003520    0.548449              1.0   \n",
       "11497     0.166667     0.782609   0.003179    0.571942              1.0   \n",
       "11498     0.166667     0.826087   0.003235    0.585708              1.0   \n",
       "11499     0.166667     0.869565   0.002908    0.561554              1.0   \n",
       "\n",
       "       trend_aroon_up  momentum_stoch_rsi  others_dr   lr_high  lr_close  \\\n",
       "11495           0.375                 1.0   0.495664  0.513635  0.541819   \n",
       "11496           1.000                 1.0   0.487706  0.486908  0.533926   \n",
       "11497           1.000                 1.0   0.521020  0.559102  0.566811   \n",
       "11498           1.000                 1.0   0.494305  0.549192  0.540472   \n",
       "11499           1.000                 1.0   0.484602  0.463870  0.530843   \n",
       "\n",
       "       r_volume  macd_wolfpack  \n",
       "11495  0.812224       0.784848  \n",
       "11496  0.782758       0.797967  \n",
       "11497  0.913431       0.900446  \n",
       "11498  0.811583       0.925196  \n",
       "11499  0.773790       0.884411  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77071777-9faf-4f19-9f3e-06d52d4805d5",
   "metadata": {
    "id": "77071777-9faf-4f19-9f3e-06d52d4805d5"
   },
   "source": [
    "# Defining the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc542ba-8365-446c-874d-48d20fb68290",
   "metadata": {},
   "source": [
    "## SimpleTradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5e14536-e09b-4a1e-b478-9d2c32fc67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# infinite number in python\n",
    "MAX_NET_WORTH = 2147483647\n",
    "MAX_NUM_QUOTE_OR_BASE_ASSET = 2147483647\n",
    "\n",
    "INITIAL_QUOTE_ASSET = 10000\n",
    "INITIAL_BASE_ASSET = 0\n",
    "OBSERVATION_WINDOW_SIZE = 24 # Probably we should put it as param ?\n",
    "\n",
    "class SimpleTradingEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['live', 'human', 'none']}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \n",
    "        self.df_scaled = config.get(\"df_scaled\").reset_index(drop=True)\n",
    "        self.df_normal = config.get(\"df_normal\").reset_index(drop=True)\n",
    "        self.window_size = OBSERVATION_WINDOW_SIZE\n",
    "        self.prices, self.features = self._process_data(self.df_scaled)\n",
    "        # The shape of the observation is (window_size * features + environment_features) the environment_features are: quote_asset, base_asset, net_worth. The entire observation is flattened in a 1D np array. \n",
    "        # NOT USED ANYMORE, KEPT FOR REFERENCE\n",
    "        # self.obs_shape = ((OBSERVATION_WINDOW_SIZE * self.features.shape[1] + 3),) \n",
    "\n",
    "        # The shape of the observation is number of candles to look back, and the number of features (candle_features) + 3 (quote_asset, base_asset, net_worth)\n",
    "        self.obs_shape = (OBSERVATION_WINDOW_SIZE, self.features.shape[1] + 3)\n",
    "\n",
    "        # Action space\n",
    "        #self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3.0, 1.0]), dtype=np.float32)\n",
    "        self.action_space = spaces.MultiDiscrete([3, 100])\n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=self.obs_shape, dtype=np.float32)\n",
    "\n",
    "        # Initialize the episode environment\n",
    "\n",
    "        self._start_candle = OBSERVATION_WINDOW_SIZE # We assume that the first observation is not the first row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "        self._end_candle = len(self.features) - 1\n",
    "        self._trading_fee = config.get(\"trading_fee\")\n",
    "\n",
    "        self._quote_asset = None\n",
    "        self._base_asset = None\n",
    "        self._done = None\n",
    "        self._current_candle = None\n",
    "        self._net_worth = None\n",
    "        self._previous_net_worth = None\n",
    "\n",
    "        # Array that will contain observation history needed for appending it to the observation space\n",
    "        # It will contain observations consisting of the net_worth, base_asset and quote_asset as list of floats\n",
    "        # Other features (OHLC + Indicators) will be appended to the current observation in the _get_observation method that takes the data directly from the available dataframe\n",
    "        self._obs_env_history = None\n",
    "\n",
    "        # Render and analysis data\n",
    "        self._total_reward_accumulated = None\n",
    "        self.trade_history = None\n",
    "        self._first_rendering = None\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self._done = False\n",
    "        self._current_candle = self._start_candle\n",
    "        self._quote_asset = INITIAL_QUOTE_ASSET\n",
    "        self._base_asset = INITIAL_BASE_ASSET \n",
    "        self._net_worth = INITIAL_QUOTE_ASSET # at the begining our net worth is the initial quote asset\n",
    "        self._previous_net_worth = INITIAL_QUOTE_ASSET # at the begining our previous net worth is the initial quote asset\n",
    "        self._total_reward_accumulated = 0.\n",
    "        self._first_rendering = True\n",
    "        self.trade_history = []\n",
    "        self._obs_env_history = []\n",
    "        \n",
    "        self._initial_obs_data()\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        self._done = False\n",
    "        current_price = random.uniform(\n",
    "            self.df_normal.loc[self._current_candle, \"low\"], self.df_normal.loc[self._current_candle, \"high\"])\n",
    "\n",
    "\n",
    "        action_type = action[0]\n",
    "        amount = action[1] / 100\n",
    "        \n",
    "        if action_type == 0: # Buy\n",
    "            # Buy % assets\n",
    "            # Determine the maximum amount of quote asset that can be bought\n",
    "            available_amount_to_buy_with = self._quote_asset / current_price\n",
    "            # Buy only the amount that agent chose\n",
    "            assets_bought = available_amount_to_buy_with * amount\n",
    "            # Update the quote asset balance\n",
    "            self._quote_asset -= assets_bought * current_price\n",
    "            # Update the base asset\n",
    "            self._base_asset += assets_bought\n",
    "            # substract trading fee from base asset based on the amount bought\n",
    "            self._base_asset -= self._trading_fee * assets_bought\n",
    "\n",
    "            # Add to trade history the amount bought if greater than 0\n",
    "            if assets_bought > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Buy', 'amount': assets_bought, 'price': current_price, 'total' : assets_bought * current_price, 'percent_amount': action[1]})\n",
    "        \n",
    "\n",
    "        elif action_type == 1: # Sell\n",
    "            # Sell % assets\n",
    "            # Determine the amount of base asset that can be sold\n",
    "            amount_to_sell = self._base_asset * amount\n",
    "            received_quote_asset = amount_to_sell * current_price\n",
    "            # Update the quote asset\n",
    "            self._quote_asset += received_quote_asset\n",
    "            # Update the base asset\n",
    "            self._base_asset -= amount_to_sell\n",
    "            \n",
    "            # substract trading fee from quote asset based on the amount sold\n",
    "            self._quote_asset -= self._trading_fee * received_quote_asset\n",
    "\n",
    "            # Add to trade history the amount sold if greater than 0\n",
    "            if amount_to_sell > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Sell', 'amount': amount_to_sell, 'price': current_price, 'total' : received_quote_asset, 'percent_amount': action[1]})\n",
    "\n",
    "        else:\n",
    "            # Hold\n",
    "            self.trade_history.append({'step': self._current_candle, 'type': 'Hold', 'amount': '0', 'price': current_price, 'total' : 0, 'percent_amount': action[1]})\n",
    "\n",
    "\n",
    "        # Update the current net worth\n",
    "        self._net_worth = self._base_asset * current_price + self._quote_asset\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns the next observation, reward, done and info.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._take_action(action)\n",
    "\n",
    "        # Calculate reward comparing the current net worth with the previous net worth\n",
    "        reward = self._net_worth - self._previous_net_worth\n",
    "\n",
    "        self._total_reward_accumulated += reward\n",
    "\n",
    "        # Update the previous net worth to be the current net worth after the reward has been applied\n",
    "        self._previous_net_worth = self._net_worth\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        # Update the info and add it to history data\n",
    "        info = dict (\n",
    "            total_reward_accumulated = self._total_reward_accumulated,\n",
    "            net_worth = self._net_worth,\n",
    "            last_action_type = self.trade_history[-1]['type'] if len(self.trade_history) > 0 else None,\n",
    "            last_action_amount = self.trade_history[-1]['amount'] if len(self.trade_history) > 0 else None,\n",
    "            current_step = self._current_candle\n",
    "        )\n",
    "\n",
    "        self._current_candle += 1\n",
    "\n",
    "        # Update observation history\n",
    "        self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n",
    "\n",
    "        self._done = self._net_worth <= 0 or self._current_candle >= (len(\n",
    "            self.df_normal.loc[:, 'open'].values) - 30)# We assume that the last observation is not the last row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "\n",
    "        if self._done:\n",
    "            print('I have finished the episode')\n",
    "        \n",
    "        return obs, reward, self._done, info\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation.\n",
    "        \"\"\"\n",
    "        data_frame = self.features[(self._current_candle - self.window_size):self._current_candle]\n",
    "\n",
    "        obs_env_history = np.array(self._obs_env_history).astype(np.float32)\n",
    "\n",
    "        #TODO We definetely need to scale the observation history in a better way, this might influence training results\n",
    "        # Doing it ad-hoc might change the scale of the min and max, thus changing the results\n",
    "        obs_env_history = preprocessing.minmax_scale(obs_env_history, (-0.9,0.9)) \n",
    "\n",
    "        obs = np.hstack((data_frame, obs_env_history[(self._current_candle - self.window_size):self._current_candle]))\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def render(self, mode='human', **kwargs):\n",
    "        \"\"\"\n",
    "        Renders a plot with trades made by the agent.\n",
    "        \"\"\"\n",
    "        \n",
    "        if mode == 'human':\n",
    "            print(f'Accumulated Reward: {self._total_reward_accumulated} ---- Current Net Worth: {self._net_worth}')\n",
    "            print(f'Current Quote asset: {self._quote_asset} ---- Current Base asset: {self._base_asset}')\n",
    "            print(f'Number of trades: {len(self.trade_history)}')\n",
    "        \n",
    "            if(len(self.trade_history) > 0):\n",
    "                print(f'Last Action: {self.trade_history[-1][\"type\"]} {self.trade_history[-1][\"amount\"]} assets ({self.trade_history[-1][\"percent_amount\"]} %) at price {self.trade_history[-1][\"price\"]}, total: {self.trade_history[-1][\"total\"]}')\n",
    "            print(f'--------------------------------------------------------------------------------------')\n",
    "        elif mode == 'live':\n",
    "            pass\n",
    "            # if self.visualization == None:\n",
    "            #     self.visualization = LiveTradingGraph(self.df_normal, kwargs.get('title', None))\n",
    "\n",
    "            # if self._current_candle > OBSERVATION_WINDOW_SIZE:\n",
    "            #     self.visualization.render(self._current_candle, self._net_worth, self.trade_history, window_size=OBSERVATION_WINDOW_SIZE)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualization != None:\n",
    "            self.visualization.close()\n",
    "            self.visualization = None\n",
    "         \n",
    "\n",
    "    def _process_data(self, df_scaled):\n",
    "        \"\"\"\n",
    "        Processes the dataframe into features.\n",
    "        \"\"\"\n",
    "        \n",
    "        prices = self.df_scaled.loc[:, 'close'].to_numpy(dtype=np.float32)\n",
    "\n",
    "        data_frame = df_scaled.iloc[:, 1:] # drop first column which is date TODO: Should be probably fixed outside of this class\n",
    "        # Convert df to numpy array\n",
    "        return prices, data_frame.to_numpy(dtype=np.float32)\n",
    "\n",
    "    def _initial_obs_data(self):\n",
    "        for i in range(self.window_size - len(self._obs_env_history)):\n",
    "            self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d40cfe",
   "metadata": {},
   "source": [
    "## BTC Accumulation env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "993ba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# infinite number in python\n",
    "MAX_NET_WORTH = 2147483647\n",
    "MAX_NUM_QUOTE_OR_BASE_ASSET = 2147483647\n",
    "\n",
    "INITIAL_QUOTE_ASSET = 0\n",
    "INITIAL_BASE_ASSET = 1\n",
    "OBSERVATION_WINDOW_SIZE = 24 # Probably we should put it as param ?\n",
    "\n",
    "class BTCAccumulationEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['live', 'human', 'none']}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \n",
    "        self.df_scaled = config.get(\"df_scaled\").reset_index(drop=True)\n",
    "        self.df_normal = config.get(\"df_normal\").reset_index(drop=True)\n",
    "        self.window_size = OBSERVATION_WINDOW_SIZE\n",
    "        self.prices, self.features = self._process_data(self.df_scaled)\n",
    "        # The shape of the observation is (window_size * features + environment_features) the environment_features are: quote_asset, base_asset, net_worth. The entire observation is flattened in a 1D np array. \n",
    "        # NOT USED ANYMORE, KEPT FOR REFERENCE\n",
    "        # self.obs_shape = ((OBSERVATION_WINDOW_SIZE * self.features.shape[1] + 3),) \n",
    "\n",
    "        # The shape of the observation is number of candles to look back, and the number of features (candle_features) + 3 (quote_asset, base_asset, net_worth)\n",
    "        self.obs_shape = (OBSERVATION_WINDOW_SIZE, self.features.shape[1] + 3)\n",
    "\n",
    "        # Action space\n",
    "        #self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([3.0, 1.0]), dtype=np.float32)\n",
    "        self.action_space = spaces.MultiDiscrete([3, 100])\n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=self.obs_shape, dtype=np.float32)\n",
    "\n",
    "        # Initialize the episode environment\n",
    "\n",
    "        self._start_candle = OBSERVATION_WINDOW_SIZE # We assume that the first observation is not the first row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "        self._end_candle = len(self.features) - 1\n",
    "        self._trading_fee = config.get(\"trading_fee\")\n",
    "\n",
    "        self._quote_asset = None\n",
    "        self._base_asset = None\n",
    "        self._done = None\n",
    "        self._current_candle = None\n",
    "        self._net_worth = None\n",
    "        self._previous_net_worth = None\n",
    "        self._previous_base_asset = None\n",
    "        self._previous_quote_asset = None\n",
    "\n",
    "        # Array that will contain observation history needed for appending it to the observation space\n",
    "        # It will contain observations consisting of the net_worth, base_asset and quote_asset as list of floats\n",
    "        # Other features (OHLC + Indicators) will be appended to the current observation in the _get_observation method that takes the data directly from the available dataframe\n",
    "        self._obs_env_history = None\n",
    "\n",
    "        # Render and analysis data\n",
    "        self._total_reward_accumulated = None\n",
    "        self.trade_history = None\n",
    "        self._first_rendering = None\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self._done = False\n",
    "        self._current_candle = self._start_candle\n",
    "        self._quote_asset = INITIAL_QUOTE_ASSET\n",
    "        self._base_asset = INITIAL_BASE_ASSET \n",
    "        self._net_worth = INITIAL_QUOTE_ASSET # at the begining our net worth is the initial quote asset\n",
    "        self._previous_net_worth = INITIAL_QUOTE_ASSET # at the begining our previous net worth is the initial quote asset\n",
    "        self._previous_base_asset = INITIAL_BASE_ASSET\n",
    "        self._previous_quote_asset = INITIAL_QUOTE_ASSET\n",
    "        self._total_reward_accumulated = 0\n",
    "        self._first_rendering = True\n",
    "        self.trade_history = []\n",
    "        self._obs_env_history = []\n",
    "        \n",
    "        self._initial_obs_data()\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        self._done = False\n",
    "        current_price = random.uniform(\n",
    "            self.df_normal.loc[self._current_candle, \"low\"], self.df_normal.loc[self._current_candle, \"high\"])\n",
    "\n",
    "\n",
    "        action_type = action[0]\n",
    "        amount = action[1] / 100\n",
    "        \n",
    "        if action_type == 0: # Buy\n",
    "            # Buy % assets\n",
    "            # Determine the maximum amount of quote asset that can be bought\n",
    "            available_amount_to_buy_with = self._quote_asset / current_price\n",
    "            # Buy only the amount that agent chose\n",
    "            assets_bought = available_amount_to_buy_with * amount\n",
    "            # Update the quote asset balance\n",
    "            self._quote_asset -= assets_bought * current_price\n",
    "            # Update the base asset\n",
    "            self._base_asset += assets_bought\n",
    "            # substract trading fee from base asset based on the amount bought\n",
    "            self._base_asset -= self._trading_fee * assets_bought\n",
    "\n",
    "            # Add to trade history the amount bought if greater than 0\n",
    "            if assets_bought > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Buy', 'amount': assets_bought, 'price': current_price, 'total' : assets_bought * current_price, 'percent_amount': action[1]})\n",
    "        \n",
    "\n",
    "        elif action_type == 1: # Sell\n",
    "            # Sell % assets\n",
    "            # Determine the amount of base asset that can be sold\n",
    "            amount_to_sell = self._base_asset * amount\n",
    "            received_quote_asset = amount_to_sell * current_price\n",
    "            # Update the quote asset\n",
    "            self._quote_asset += received_quote_asset\n",
    "            # Update the base asset\n",
    "            self._base_asset -= amount_to_sell\n",
    "            \n",
    "            # substract trading fee from quote asset based on the amount sold\n",
    "            self._quote_asset -= self._trading_fee * received_quote_asset\n",
    "\n",
    "            # Add to trade history the amount sold if greater than 0\n",
    "            if amount_to_sell > 0:\n",
    "                self.trade_history.append({'step': self._current_candle, 'type': 'Sell', 'amount': amount_to_sell, 'price': current_price, 'total' : received_quote_asset, 'percent_amount': action[1]})\n",
    "\n",
    "        else:\n",
    "            # Hold\n",
    "            self.trade_history.append({'step': self._current_candle, 'type': 'Hold', 'amount': '0', 'price': current_price, 'total' : 0, 'percent_amount': action[1]})\n",
    "\n",
    "\n",
    "        # Update the current net worth\n",
    "        self._net_worth = self._base_asset * current_price + self._quote_asset\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns the next observation, reward, done and info.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._take_action(action)\n",
    "\n",
    "        # Calculate reward comparing the current base asset with the previous base asset\n",
    "        reward = self._base_asset - self._previous_base_asset\n",
    "\n",
    "        self._total_reward_accumulated += reward\n",
    "\n",
    "        # Update the previous net worth to be the current net worth after the reward has been applied\n",
    "        self._previous_net_worth = self._net_worth\n",
    "        self._previous_base_asset = self._base_asset\n",
    "        self._previous_quote_asset = self._quote_asset\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        # Update the info and add it to history data\n",
    "        info = dict (\n",
    "            total_reward_accumulated = self._total_reward_accumulated,\n",
    "            net_worth = self._net_worth,\n",
    "            last_action_type = self.trade_history[-1]['type'] if len(self.trade_history) > 0 else None,\n",
    "            last_action_amount = self.trade_history[-1]['amount'] if len(self.trade_history) > 0 else None,\n",
    "            current_step = self._current_candle\n",
    "        )\n",
    "\n",
    "        self._current_candle += 1\n",
    "\n",
    "        # Update observation history\n",
    "        self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n",
    "\n",
    "        self._done = self._net_worth <= 0 or self._current_candle >= (len(\n",
    "            self.df_normal.loc[:, 'open'].values) - 30)# We assume that the last observation is not the last row of the dataframe, in order to avoid the case where there are no calculated indicators.\n",
    "\n",
    "        if self._done:\n",
    "            print('The episode has finished')\n",
    "        \n",
    "        return obs, reward, self._done, info\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation.\n",
    "        \"\"\"\n",
    "        data_frame = self.features[(self._current_candle - self.window_size):self._current_candle]\n",
    "\n",
    "        obs_env_history = np.array(self._obs_env_history).astype(np.float32)\n",
    "\n",
    "        #TODO We definetely need to scale the observation history in a better way, this might influence training results\n",
    "        # Doing it ad-hoc might change the scale of the min and max, thus changing the results\n",
    "        obs_env_history = preprocessing.minmax_scale(obs_env_history, (-0.9,0.9)) \n",
    "\n",
    "        obs = np.hstack((data_frame, obs_env_history[(self._current_candle - self.window_size):self._current_candle]))\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def render(self, mode='human', **kwargs):\n",
    "        \"\"\"\n",
    "        Renders a plot with trades made by the agent.\n",
    "        \"\"\"\n",
    "        \n",
    "        if mode == 'human':\n",
    "            print(f'Accumulated Reward: {self._total_reward_accumulated} ---- Current Net Worth: {self._net_worth}')\n",
    "            print(f'Current Quote asset: {self._quote_asset} ---- Current Base asset: {self._base_asset}')\n",
    "            print(f'Number of trades: {len(self.trade_history)}')\n",
    "        \n",
    "            if(len(self.trade_history) > 0):\n",
    "                print(f'Last Action: {self.trade_history[-1][\"type\"]} {self.trade_history[-1][\"amount\"]} assets ({self.trade_history[-1][\"percent_amount\"]} %) at price {self.trade_history[-1][\"price\"]}, total: {self.trade_history[-1][\"total\"]}')\n",
    "            print(f'--------------------------------------------------------------------------------------')\n",
    "        elif mode == 'live':\n",
    "            pass\n",
    "            # if self.visualization == None:\n",
    "            #     self.visualization = LiveTradingGraph(self.df_normal, kwargs.get('title', None))\n",
    "\n",
    "            # if self._current_candle > OBSERVATION_WINDOW_SIZE:\n",
    "            #     self.visualization.render(self._current_candle, self._net_worth, self.trade_history, window_size=OBSERVATION_WINDOW_SIZE)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualization != None:\n",
    "            self.visualization.close()\n",
    "            self.visualization = None\n",
    "         \n",
    "\n",
    "    def _process_data(self, df_scaled):\n",
    "        \"\"\"\n",
    "        Processes the dataframe into features.\n",
    "        \"\"\"\n",
    "        \n",
    "        prices = self.df_scaled.loc[:, 'close'].to_numpy(dtype=np.float32)\n",
    "\n",
    "        data_frame = df_scaled.iloc[:, 1:] # drop first column which is date TODO: Should be probably fixed outside of this class\n",
    "        # Convert df to numpy array\n",
    "        return prices, data_frame.to_numpy(dtype=np.float32)\n",
    "\n",
    "    def _initial_obs_data(self):\n",
    "        for i in range(self.window_size - len(self._obs_env_history)):\n",
    "            self._obs_env_history.append([self._net_worth, self._base_asset, self._quote_asset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbef5bd",
   "metadata": {},
   "source": [
    "### Create a vectorized ENV as wrapper for parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b670e8a2",
   "metadata": {},
   "source": [
    "### Init ray and trainer config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc0188",
   "metadata": {},
   "source": [
    "### How to setup a cluster fast:\n",
    "\n",
    "1. You need to decide which PC/server will be the head node and just run ``` ray start --head --port 6379 ```, this will be the master node\n",
    "\n",
    "2. On the worker nodes you need to make sure that\n",
    "a. You have the same ray and python version\n",
    "b. You have installed all needed pip deps:\n",
    "```pip install ray torch torchvision tabulate tensorboard tensorflow sklearn```\n",
    "c. You have network conectivity to the head node (```telnet head-node-ip 6379```)\n",
    "3. On all the machines that you want them to act as worker nodes you run :\n",
    "```ray start --address=\"192.168.0.206:6379\" --node-ip-address=\"192.168.0.150\" --num-cpus=4```\n",
    "a. address is the head node ip\n",
    "b. node ip address is the ip of the worker node, pay attention to use an external one not localhost or 127.0.0.1\n",
    "c. num cpus - how much cpu power you want to alocate\n",
    "4. Also you need to make sure that your worker nodes are reacheable from the head node, because the communication is from both sides\n",
    "\n",
    "That's all :) Enjoy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c5ccf",
   "metadata": {},
   "source": [
    "Very important notes about parallelism :\n",
    "\n",
    "1. ```num_worker``` is per trial ! so if you increase this you will eat up resources and won't be able to scale to multiple parallel trials when doing hyperparam tuning, see below\n",
    "2. a trial uses 2 CPU resources to run (1 for the driver ```num_cpus_for_driver``` -> and 1 for the worker (```num_cpus_per_worker```)). \n",
    "3. a trial has only 1 driver, while it can have multiple workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9c67124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_optimal_resource_allocation(available_cpu, available_gpu):\n",
    "    \"\"\"\n",
    "    Finds the optimal resource allocation for the agent based on the available resources in the cluster\n",
    "    \"\"\"\n",
    "    # If we have GPU available, we allocate it all for the training, while creating as much workers as CPU cores we have minus one for the driver which holds the trainer\n",
    "    if available_gpu > 0:\n",
    "        return {\n",
    "            'num_workers': available_cpu - 1,\n",
    "            'num_cpus_per_worker': 1,\n",
    "            'num_envs_per_worker': 1,\n",
    "            'num_gpus_per_worker': 0,\n",
    "            'num_cpus_for_driver': 1,\n",
    "            'num_gpus' : available_gpu\n",
    "        }\n",
    "    # If we don't have GPU available, we allocate as much CPU cores for the training as possible, while creating only one worker for stepping the environment\n",
    "    else:\n",
    "        # according to the benchmark, we should allocate more workers, each with 1 cpu, letting the rest for the driver\n",
    "        num_workers = int(math.floor((available_cpu  * 90) / 100))\n",
    "        num_envs_per_worker = 1\n",
    "        num_cpu_for_driver = available_cpu - num_workers - num_envs_per_worker - 4 # -4 unkown why\n",
    "        return {\n",
    "            'num_workers': num_workers,\n",
    "            'num_cpus_per_worker': 1, # this should be enough for stepping an env at once\n",
    "            'num_envs_per_worker': num_envs_per_worker, # it doesn't seem to add any benefits to have more than one env per worker\n",
    "            'num_gpus_per_worker': 0, # the inference is done pretty fast, so there is no need to use GPU, at least not when we run one trial at once\n",
    "            'num_cpus_for_driver': num_cpu_for_driver,\n",
    "            'evaluation_num_workers' : 1,\n",
    "            'num_gpus' : 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43f5ba0a-89dd-4c89-aa81-a52c27301677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ray\n",
    "import os\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.env.vector_env import VectorEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "\n",
    "# Get the current working directory and create a folder to store the results\n",
    "# cwd = os.getcwd()\n",
    "# local_dir = \"~/ray_results/\"\n",
    "# if not os.path.exists(local_dir):\n",
    "#     os.makedirs(local_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Let's define some tuning parameters\n",
    "FC_SIZE = tune.grid_search([[256, 256], [1024], [128, 64, 32]])  \n",
    "LEARNING_RATE = tune.grid_search([0.001, 0.0005, 0.00001])\n",
    "MINIBATCH_SIZE = tune.grid_search([5, 10, 20])  \n",
    "GAMMA = tune.grid_search([0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "\n",
    "\n",
    "# Initialize Ray\n",
    "ray.shutdown() # let's shutdown first any running instances of ray (don't confuse it with the cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f9a7192-eeef-4a84-9441-fe546ee775dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClientContext(dashboard_url='192.168.178.59:8265', python_version='3.9.5', ray_version='1.12.0', ray_commit='f18fc31c7562990955556899090f8e8656b48d2d', protocol_version='2022-03-16', _num_clients=1, _context_to_restore=<ray.util.client._ClientContext object at 0x7f2c884159d0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['RAY_record_ref_creation_sites'] = '1' # Needed for debugging when things go wrong\n",
    "\n",
    "#added by p-pl\n",
    "ray.init(address='ray://192.168.178.59:10001') \n",
    "#ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c07de6fd-c203-4457-89af-76b8cfd573ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpu_in_cluster = 0 #ray.available_resources()['GPU'] if ray.available_resources()['GPU']  else 0\n",
    "available_cpu_in_cluster = ray.available_resources()['CPU'] if ray.available_resources()['CPU']  else 0\n",
    "\n",
    "# In the first version we assume that we have only one node cluster, so the allocation logic is based on that\n",
    "# So the resources are maximized for one ray tune trial at a time\n",
    "parallel_config = find_optimal_resource_allocation(available_cpu_in_cluster, available_gpu_in_cluster) # Currently we are going to disable GPU ussage due to it's poor performance on a single instance cluster\n",
    "\n",
    "trading_fee = 0.0075\n",
    "training_config = {\n",
    "            \"trading_fee\": trading_fee,\n",
    "            \"df_normal\": X_train,\n",
    "            \"df_scaled\": X_train_scaled,\n",
    "}\n",
    "\n",
    "eval_config = {\n",
    "            \"trading_fee\": trading_fee,\n",
    "            \"df_normal\": X_test,\n",
    "            \"df_scaled\": X_test_scaled,\n",
    "}\n",
    "\n",
    "training_env = SimpleTradingEnv(training_config)\n",
    "eval_env = SimpleTradingEnv(eval_config)\n",
    "\n",
    "training_env_key = \"SimpleTradingEnv-training-V01\"\n",
    "eval_env_key = \"SimpleTradingEnv-evaluating-V01\"\n",
    "\n",
    "# tune.register_env(\"SimpleTradingEnv-training-V01\", lambda _: training_env)\n",
    "# tune.register_env(\"SimpleTradingEnv-evaluating-V01\", lambda _: eval_env)\n",
    "\n",
    "# training_env = BTCAccumulationEnv(training_config)\n",
    "# eval_env = BTCAccumulationEnv(eval_config)\n",
    "\n",
    "# training_env_key = \"BTCAccumulationEnv-training-V01\"\n",
    "# eval_env_key = \"BTCAccumulationEnv-evaluating-V01\"\n",
    "\n",
    "tune.register_env(training_env_key, lambda _: training_env)\n",
    "tune.register_env(eval_env_key, lambda _: eval_env)\n",
    "\n",
    "\n",
    "# Create the ppo trainer configuration\n",
    "ddppo_trainer_config = {\n",
    "        \"env\": training_env_key, # Ray will automatically create multiple environments and vectorize them if needed\n",
    "        \"horizon\": len(X_train_scaled) - 30,\n",
    "        \"log_level\": \"WARN\",\n",
    "        #\"framework\": \"tf\",\n",
    "        \"framework\": \"torch\", #p-pl\n",
    "        #\"eager_tracing\": True,\n",
    "        \"ignore_worker_failures\": True, \n",
    "        \"num_workers\": parallel_config.get(\"num_workers\"), # Number of workers is per trial run, so the more we put the less parallelism we have\n",
    "        \"num_envs_per_worker\": parallel_config.get(\"num_envs_per_worker\"), # This influences also the length of the episode. the environment length will be split by the number of environments per worker\n",
    "        \"num_gpus\": parallel_config.get(\"num_gpus\"), # Number of GPUs to use in training (0 means CPU only). After a few experiments, it seems that using GPU is not helping\n",
    "        \"num_cpus_per_worker\": parallel_config.get(\"num_cpus_per_worker\"), # After some testing, seems the fastest way for this kind of enviroment. It's better to run more trials in parallel than to finish a trial with a couple of minutes faster. Because we can end trial earlier if we see that our model eventuall converge\n",
    "        \"num_cpus_for_driver\": parallel_config.get(\"num_cpus_for_driver\"), # Number of CPUs to use for the driver. This is the number of CPUs used for the training process.\n",
    "        \"num_gpus_per_worker\": parallel_config.get(\"num_gpus_per_worker\"), \n",
    "        \"evaluation_num_workers\": 5, #p-pl\n",
    "        \"torch_distributed_backend\": \"gloo\", #p-pl\n",
    "        \"truncate_episodes\": True, #p-pl\n",
    "        \"rollout_fragment_length\": 200, # Size of batches collected from each worker. If num_envs_per_worker is > 1 the rollout value will be multiplied by num_envs_per_worker\n",
    "        \"train_batch_size\": 2048, # Number of timesteps collected for each SGD round. This defines the size of each SGD epoch. the batch size is composed of fragments defined above\n",
    "        \"sgd_minibatch_size\": 64,\n",
    "            \"train_batch_size\": -1, #p-pl\n",
    "        #p-pl \"batch_mode\": \"complete_episodes\",\n",
    "        \"vf_clip_param\": 100, # Default is 10, but we increase it to 100 to adapt it to our rewards scale. It helps our value function to converge faster\n",
    "        \"lr\": 0.00001,  # Hyperparameter grid search defined above\n",
    "        \"gamma\": 0.95,  # This can have a big impact on the result and needs to be properly tuned\n",
    "        #\"observation_filter\": \"MeanStdFilter\",\n",
    "        \"model\": {\n",
    "        #    \"fcnet_hiddens\": FC_SIZE,  # Hyperparameter grid search defined above\n",
    "            # \"use_lstm\": True,\n",
    "            # \"lstm_cell_size\": 256,\n",
    "            # \"lstm_use_prev_action_reward\": True,\n",
    "            # \"lstm_use_prev_action\": True,\n",
    "            \n",
    "        },\n",
    "        #\"sgd_minibatch_size\": MINIBATCH_SIZE,  # Hyperparameter grid search defined above\n",
    "        \"evaluation_interval\": 5,  # Run one evaluation step on every x `Trainer.train()` call.\n",
    "        \"evaluation_duration\": 1,  # How many episodes to run evaluations for each time we evaluate.\n",
    "        \"evaluation_config\": {\n",
    "            \"explore\": True,  # We usually don't want to explore during evaluation. All actions have to be repeatable. Similar to deterministic = True, but on-policy algorithms can get better results with exploration.\n",
    "            \"env\": eval_env_key, # We need to define a new environment for evaluation with different parameters\n",
    "        },\n",
    "        \"logger_config\": {\n",
    "            \"logdir\": \"/tmp/ray_logging/\",\n",
    "            \"type\": \"ray.tune.logger.UnifiedLogger\",\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cfc82",
   "metadata": {},
   "source": [
    "### Print some info about all nodes in cluster, just to make sure we really are in a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f0c8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU in cluster:  64.0\n",
      "Available CPU in cluster:  64.0\n",
      "Existing nodes in cluster:  [{'NodeID': 'ea676f696251bbf0a4ab67de755ec1f25f216d1d2697fcc113b068ec', 'Alive': True, 'NodeManagerAddress': '192.168.178.59', 'NodeManagerHostname': 'i1', 'NodeManagerPort': 33443, 'ObjectManagerPort': 42983, 'ObjectStoreSocketName': '/tmp/ray/session_2022-05-08_09-42-07_551961_172101/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2022-05-08_09-42-07_551961_172101/sockets/raylet', 'MetricsExportPort': 44540, 'alive': True, 'Resources': {'node:192.168.178.59': 1.0, 'CPU': 64.0, 'object_store_memory': 80682856857.0, 'memory': 178259999335.0}}]\n",
      "RAY_record_ref_creation_sites:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'192.168.178.59'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "print ('Total CPU in cluster: ', ray.cluster_resources()['CPU'])\n",
    "print ('Available CPU in cluster: ', ray.available_resources()['CPU'])\n",
    "#print('Total GPU in cluster: ', ray.cluster_resources()['GPU'])\n",
    "#print ('Available GPU in cluster: ', ray.available_resources()['GPU'])\n",
    "print('Existing nodes in cluster: ', ray.nodes())\n",
    "print('RAY_record_ref_creation_sites: ', os.environ.get('RAY_record_ref_creation_sites'))\n",
    "\n",
    "# This ensures that remote cluster is working\n",
    "@ray.remote\n",
    "def f():\n",
    "    time.sleep(0.01)\n",
    "    return ray._private.services.get_node_ip_address()\n",
    "\n",
    "# Get a list of the IP addresses of the nodes that have joined the cluster.\n",
    "set(ray.get([f.remote() for _ in range(1000)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a77757",
   "metadata": {},
   "source": [
    "### Run ray tune "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbba6c41-4bdc-4494-845c-38f310b5b812",
   "metadata": {},
   "source": [
    "to monitor running ray:\n",
    "http://[head_node_ray_cluster]:8265/\n",
    "\n",
    "to monitor tuning of the model:\n",
    "start tensorboard with: tensorboard --logdir=\"/tmp/ray_results/\" --host=0.0.0.0\n",
    "http://[head_node_ray_cluster]:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508d31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-05-08 10:05:49 (running for 00:18:52.09)<br>Memory usage on this node: 23.2/251.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 64.0/64 CPUs, 0/0 GPUs, 0.0/166.02 GiB heap, 0.0/75.14 GiB objects<br>Result logdir: /tmp/ray_results/Bitcoin_Acc_Local_DDPPO_Trial_V02<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                     </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPPO_SimpleTradingEnv-training-V01_08bcc_00000</td><td>RUNNING </td><td>192.168.178.59:179411</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1033.26</td><td style=\"text-align: right;\">558600</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-06-15\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.985370519264\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.985370519264\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.985370519264\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.985370519264\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08326676130980096\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.4905424540884455\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4773362033642794\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13168819559233783\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.08517775535583497\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.703201305866242\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.04160367741715163\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.04431821834295988\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.522172836959362\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.0027145419211592526\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 570000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 570000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 50\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 9.86923076923077\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1124.7864937782288\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 91.52814030647278\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1124.7864937782288\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7323.338\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997175\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 10000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 570000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 50\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-06-22\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07339601516723633\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7032007575035095\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.013066488702315837\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.016204801748972385\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9767007946968078\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.003138308218331076\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 581400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 581400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 51\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.92727272727274\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1132.2390036582947\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.452509880065918\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1132.2390036582947\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7360.895\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997182\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 10200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 581400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 51\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-06-30\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06207748055458069\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.703173744678497\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.014182553137652576\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.017444007308222355\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.998727148771286\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.0032614545081742106\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 592800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 592800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 52\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 77.60909090909091\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1139.7307538986206\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.491750240325928\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1139.7307538986206\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7407.085\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997190\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 10400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 592800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 52\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-06-38\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07155155539512634\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.703208971023559\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.014618526352569461\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.012220105296000839\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.6420454114675522\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.0023984187573660165\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 604200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 604200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 53\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 75.08181818181819\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1147.5253512859344\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.794597387313843\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1147.5253512859344\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7455.751\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997198\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 10600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 604200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 53\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-06-46\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.08243088126182556\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.703058505058289\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.003973191836848855\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.00030728522688150406\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.966609163582325\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.003665905987145379\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 615600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 615600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 54\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 75.69090909090909\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1155.2887861728668\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.763434886932373\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1155.2887861728668\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7514.778\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997206\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 10800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 615600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 54\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-08-17\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.974129891696\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.974129891696\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.974129891696\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.974129891696\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08331540626795468\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.494329981422257\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4774399876402344\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.131707824046177\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06730726957321168\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.703044652938843\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.029227344365790487\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.026069678412750364\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.4741585448384285\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.003157663409365341\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 627000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 627000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 55\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 10.195384615384615\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1246.290688276291\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 91.00190210342407\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1246.290688276291\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7557.523\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997297\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 11000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 627000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 55\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-08-25\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0663103699684143\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.703049612045288\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.011583106638863683\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.014777474710717798\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9524893909692764\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.00319436703575775\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 638400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 638400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 56\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.53636363636365\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1254.1082470417023\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.817558765411377\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1254.1082470417023\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7573.81\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997305\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 11200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 638400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 56\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-08-33\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: .nan\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.058871716260910034\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702982807159424\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.029671701381448656\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.026716014300473035\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8788753241300583\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.0029556831053923816\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 649800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 649800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 79.48181818181818\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1261.779456615448\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.6712095737457275\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1261.779456615448\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7603.857\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997313\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 11400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 649800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180264)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180246)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180282)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180260)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180291)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180242)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180257)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180243)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180244)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180316)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180298)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180354)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180262)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180302)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180283)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180281)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180265)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180374)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180300)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180360)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180263)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180394)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180392)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180356)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180338)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180359)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180318)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180241)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180376)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180284)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180355)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180299)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180320)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180335)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180321)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180239)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180334)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180375)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180340)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180319)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180279)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180261)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180245)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180280)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180240)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180301)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180266)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180317)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180357)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180353)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180358)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180378)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180373)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180379)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180336)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180303)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180377)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-08-38\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.054872405529022214\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7030264139175415\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.02081035478040576\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 74.21905355453491\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 6.279200315475463e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 74.19824342727661\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 661200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 661200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 58\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.0625\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1266.7833063602448\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.003849744796753\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1266.7833063602448\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7370.466\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997318\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 11600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 661200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 58\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-10-10\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.978690644388\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.978690644388\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.978690644388\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.978690644388\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08334833921270426\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.4978828755396965\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.477667873667213\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.1317005654328683\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.057707476615905764\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7025318264961244\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.008721261331811548\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 98.24057197570801\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 5.4776668548583984e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 98.23185214996337\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 684000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 684000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 60\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 7.231199999999999\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1358.7505373954773\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 87.65362977981567\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1358.7505373954773\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6705.749\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997410\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 12000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 684000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 60\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-10-19\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05432212948799133\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702602386474609\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.028715765848755837\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 95.68330554962158\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.00019167959690093994\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 95.65458889007569\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 706800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 706800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 62\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.69999999999999\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1367.6685438156128\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.423274993896484\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1367.6685438156128\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6106.227\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997419\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 12400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 706800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 62\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-10-28\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05558366775512695\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702933418750763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.01311471895314753\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 94.86797828674317\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.00012114346027374268\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 94.88109188079834\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 729600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 729600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 64\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.35\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1376.7673625946045\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.567027568817139\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1376.7673625946045\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5460.467\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997428\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 12800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 729600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 64\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-11-56\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.980681890042\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.980681890042\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.980681890042\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.980681890042\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08333831311909753\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.500594879131009\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4774021919342832\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13166048229395627\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05547595024108887\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7029433965682985\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.009122376376762987\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 95.79792633056641\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0002927839756011963\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 95.78880443572999\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 741000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 741000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 65\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 7.66031746031746\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1464.4844586849213\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 87.71709609031677\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1464.4844586849213\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5138.178\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997516\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 13000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 741000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 65\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-12-06\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05726925730705261\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702817237377166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.016136549902148543\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 96.57876129150391\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0005486711859703064\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 96.59489822387695\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 763800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 763800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 67\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.48571428571428\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1474.0595364570618\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.8259522914886475\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1474.0595364570618\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4547.616\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997526\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 13400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 763800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 67\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-12-12\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07963916063308715\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7028745889663695\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.01563670616596937\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 88.29836177825928\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0002734735608100891\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 88.28272399902343\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 775200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 775200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 68\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 70.76666666666667\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1479.7287051677704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.669168710708618\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1479.7287051677704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4614.248\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997532\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 13600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 775200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 68\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-12-17\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06003657579421997\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.70293105840683\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.03666452668257989\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 68.47490034103393\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0011767134070396422\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 68.51156377792358\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 786600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 786600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 69\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 71.58571428571429\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1484.7647578716278\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.036052703857422\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1484.7647578716278\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4687.52\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 13800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 786600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 69\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-13-45\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.987091273677\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.987091273677\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.987091273677\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.987091273677\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08335947309731656\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.5025338605546095\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.476727952450991\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13165537617449016\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.053751415014266966\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702898478507995\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.04038560753688216\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 74.57892971038818\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0012981429696083068\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 74.61931495666504\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 798000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 798000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 70\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 7.814285714285713\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1572.7105131149292\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 87.94575524330139\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1572.7105131149292\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4740.722\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997625\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 14000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 798000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 70\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-13-50\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.061608582735061646\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702845335006714\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.01820233042817563\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 68.07901916503906\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.00040929317474365235\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 68.09722175598145\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 809400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 809400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 71\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.61428571428571\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1577.8137545585632\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.103241443634033\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1577.8137545585632\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4801.846\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997630\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 14200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 809400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 71\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-13-56\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06159200668334961\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702827525138855\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.046445155353285375\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 52.68895139694214\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.0025127872824668883\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 52.64250659942627\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 820800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 820800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 72\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 70.9625\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1583.0564632415771\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.242708683013916\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1583.0564632415771\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4884.244\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997636\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 14400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 820800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 72\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-14-01\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.061371994018554685\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702730596065521\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.006091358209960163\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 35.533600425720216\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.004427856206893921\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 35.52750926017761\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 832200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 832200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 73\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.2875\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1588.36492228508\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.308459043502808\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1588.36492228508\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4962.176\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997641\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 14600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 832200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 73\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-14-07\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06388155817985534\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7026339888572695\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.004458745662122965\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 21.620889163017274\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.006790871918201447\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 21.62534809112549\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 843600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 843600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 74\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 71.25\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1593.8255367279053\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.460614442825317\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1593.8255367279053\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5052.084\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997647\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 14800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 843600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 74\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-15-35\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.97565975562\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.97565975562\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.97565975562\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.97565975562\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08338709765910178\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.503473171079469\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.475824839662318\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13156860396621392\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05867366194725036\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702555561065674\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.011414976534433663\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 24.24785542488098\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0013129711151123047\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 24.23643970489502\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 855000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 855000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 75\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 7.9368\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1682.0105032920837\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 88.18496656417847\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1682.0105032920837\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5131.946\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997735\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 15000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 855000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 75\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-15-41\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0586811363697052\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702434575557708\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.03361861929297447\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 8.827822881937028\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.006518200039863586\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 8.861441504955291\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 866400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 866400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 76\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 67.3375\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1687.4618155956268\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.451312303543091\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1687.4618155956268\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5201.403\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997741\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 15200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 866400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 76\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-15-46\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06498105525970459\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702347016334533\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.026491457549855112\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 8.913143527507781\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.004963226616382599\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 8.886651909351349\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 877800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 877800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 77\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.19999999999999\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1693.0920922756195\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.630276679992676\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1693.0920922756195\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5281.115\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997746\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 15400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 877800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 77\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-15-52\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06918886303901672\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7024116039276125\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.007575105829164386\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 5.719844150543213\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.003288668394088745\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 5.727419292926788\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 889200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 889200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 78\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.26666666666667\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1698.668773651123\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.57668137550354\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1698.668773651123\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5272.148\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997752\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 15600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 889200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 78\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-15-58\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0653420329093933\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702321195602417\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.013544128090143204\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 6.175633060932159\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.003968837857246399\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 6.189177167415619\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 900600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 900600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 79\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.725\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1704.4370403289795\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.768266677856445\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1704.4370403289795\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5344.337\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997758\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 15800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 900600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 79\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-17-27\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.990785041311\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.990785041311\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.990785041311\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.990785041311\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.0833979758857412\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.50542702584751\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4757954231447634\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13156337959144868\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05884250402450562\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702358984947205\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.0018513421062380075\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 2.1075256884098055\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.01536206305027008\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 2.1093770444393156\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 912000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 912000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 80\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 8.353543307086612\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1793.2321283817291\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 88.79508805274963\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1793.2321283817291\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5414.768\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997847\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 16000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 912000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 80\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-17-33\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06254860162734985\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702198028564453\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.00022589885629713534\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 1.4497955322265625\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.005946989357471466\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 1.4495696380734444\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 923400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 923400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 81\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.6875\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1799.0714573860168\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.83932900428772\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1799.0714573860168\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5487.127\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997853\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 16200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 923400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 81\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-17-39\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06554160118103028\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702235221862793\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.011499493103474378\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.48485374804586173\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.030304259061813353\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.4963532380759716\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 934800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 934800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 82\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.81111111111112\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1805.0495982170105\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.978140830993652\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1805.0495982170105\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5559.777\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997859\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 16400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 934800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 82\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-17-45\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.052965039014816286\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702526521682739\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.0007195613346993923\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.39295574836432934\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0053071662783622745\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.3922361843287945\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 946200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 946200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 83\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.4875\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1810.922144651413\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.872546434402466\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1810.922144651413\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5615.591\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997865\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 16600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 946200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 83\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-17-51\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05865090489387512\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702370810508728\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.021606945153325795\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.21022200966253876\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.026995867490768433\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.18861506469547748\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 957600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 957600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 84\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.85555555555555\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1816.911447763443\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.989303112030029\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1816.911447763443\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5668.367\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997871\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 16800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 957600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 84\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-19-20\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.985724663748\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.985724663748\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.985724663748\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.985724663748\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08341488434564197\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.506398386089307\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.475705608254351\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.1315221364365698\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.061556833982467654\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7023394227027895\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.012048719311133027\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.10082910982891917\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.05735842436552048\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.08878038804978132\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 969000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 969000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 85\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 8.694488188976377\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1905.9663898944855\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 89.05494213104248\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1905.9663898944855\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5735.305\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997960\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 17000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 969000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 85\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-19-26\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0618401825428009\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7022132396698\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.01460716649889946\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.04065649975091219\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.03525882512331009\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.02604933371767402\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 980400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 980400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 86\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 69.35555555555555\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1912.1622202396393\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.195830345153809\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1912.1622202396393\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5808.743\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997966\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 17200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 980400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 86\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-19-33\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05698512196540832\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702311503887176\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.010713129001669586\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.003978743473999203\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.07301484495401382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.006734384072478861\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 991800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 991800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 87\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 75.95555555555556\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1918.2217218875885\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.059501647949219\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1918.2217218875885\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5850.665\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997973\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 17400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 991800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 87\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-19-39\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05778413414955139\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702170979976654\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.007731889979913831\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.011649145954288542\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.3457623526453972\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.003917257345165126\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1003200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1003200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 88\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.07777777777778\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1924.5286514759064\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.306929588317871\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1924.5286514759064\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5923.699\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997979\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 17600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1003200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 88\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-19-45\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06416800022125244\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702147543430328\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.037171678803861144\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.042095930059440435\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.033906927704811095\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.004924256086815149\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1014600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1014600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 89\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.02222222222221\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 1930.8651463985443\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.3364949226379395\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 1930.8651463985443\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5982.349\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651997985\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 17800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1014600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 89\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-21-15\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.973643855807\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.973643855807\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.973643855807\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.973643855807\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08340159488228037\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.507968661925483\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4758177351781834\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13153108258961582\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05826157331466675\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702270150184631\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.016484741983003915\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.021493559959344564\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9199951887130737\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.00500881507032318\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1026000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1026000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 90\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 9.13359375\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2020.24072432518\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 89.37557792663574\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2020.24072432518\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6037.907\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998075\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 18000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1026000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 90\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-21-22\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06583328247070312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702050638198853\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.014735478884540498\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.019982306519523263\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.08169864863157272\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.005246827792143449\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1037400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1037400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 91\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.3111111111111\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2026.801364660263\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.560640335083008\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2026.801364660263\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6109.294\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998082\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 18200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1037400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 91\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-21-28\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0632313072681427\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701814067363739\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.022344238415826112\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.028182801185175778\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8829337000846863\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.005838560493430123\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1048800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1048800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 92\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.36999999999999\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2033.3415839672089\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.540219306945801\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2033.3415839672089\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6165.02\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998088\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 18400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1048800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 92\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-21-35\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.067761892080307\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701587224006653\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.02192623382434249\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.015619631926529109\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9484401807188988\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.00630660008173436\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1060200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1060200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 93\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.12222222222222\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2040.0179126262665\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.676328659057617\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2040.0179126262665\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6244.953\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998095\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 18600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1060200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 93\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-21-42\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06298995018005371\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701602530479431\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.03664598003961146\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.029498887760564686\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9217366993427276\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.007147094164974987\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1071600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1071600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 94\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 75.26000000000002\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2046.6223485469818\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.604435920715332\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2046.6223485469818\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6305.885\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998102\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 18800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1071600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 94\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-23-03\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.98308225294\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.98308225294\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.98308225294\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.98308225294\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08299810582981094\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.481040424198265\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4655070975260127\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13088649381836442\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06792092323303223\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701782143115997\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.029564982559531928\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.03990541631355882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9671851813793182\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01034043193794787\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1083000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1083000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 95\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 10.396551724137929\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2127.953057050705\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 81.33070850372314\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2127.953057050705\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6383.662\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998183\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 19000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1083000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 95\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-23-10\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06802545785903931\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701568675041199\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.019031368382275104\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.009722451586276293\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9473827123641968\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.00930891518946737\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1094400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1094400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 96\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 70.62\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2134.881668329239\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.9286112785339355\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2134.881668329239\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6459.069\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998190\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 19200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1094400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 96\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-23-17\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07080888152122497\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701826739311218\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.027587883407250047\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.03870331603102386\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9902267307043076\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.011115435278043151\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1105800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1105800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 97\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.3\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2141.778378725052\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 6.896710395812988\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2141.778378725052\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6544.826\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998197\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 19400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1105800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 97\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-23-24\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06593302488327027\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701750218868256\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.001830794382840395\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.011317909508943558\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -1.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01314870526548475\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1117200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1117200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 98\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.31\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2148.785892009735\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.0075132846832275\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2148.785892009735\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6614.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998204\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 19600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1117200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 98\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-23-32\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07014917135238648\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701848948001862\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.04007308348082006\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.05323569481261074\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8300645500421524\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.013162612891755998\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1128600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1128600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 99\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.96000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2155.8453657627106\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.059473752975464\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2155.8453657627106\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6684.651\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998212\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 19800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1128600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 99\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-25-02\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.985416392406\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.985416392406\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.985416392406\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.985416392406\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08303074049344854\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.484016682903448\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.466348716459587\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.1309411133637159\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06903876662254334\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701962268352508\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.038155630324035884\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.0521013755351305\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8124981373548508\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01394574714358896\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1140000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1140000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 100\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 9.690697674418606\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2246.2148122787476\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 90.36944651603699\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2246.2148122787476\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6771.183\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998302\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 20000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1140000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 100\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-25-09\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.066523677110672\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701629745960235\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.0032409259118139746\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.01748905787244439\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.7845888242125512\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.014248137269169093\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1151400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1151400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 101\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 68.97272727272727\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2253.4498269557953\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.2350146770477295\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2253.4498269557953\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6840.606\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998309\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 20200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1151400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 101\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-25-17\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.069959956407547\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701570773124695\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.023394226795062423\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.006959728244692087\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9966523557901382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01643449899274856\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1162800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1162800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 102\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.71818181818183\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2260.804749250412\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.354922294616699\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2260.804749250412\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6922.095\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998317\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 20400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1162800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 102\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-25-24\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06922274231910705\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701647913455963\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.04320328813046217\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.029251342313364147\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.02735697329044342\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.013951942068524658\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1174200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1174200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 103\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 75.41\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2268.1458899974823\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.3411407470703125\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2268.1458899974823\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6988.071\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998324\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 20600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1174200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 103\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-25-32\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07957313656806946\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701898682117462\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.012101460527628661\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.003562503773719072\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9692979529500008\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.0156639646505937\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1185600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1185600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 104\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.24545454545455\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2275.7622768878937\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.616386890411377\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2275.7622768878937\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7089.817\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998332\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 20800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1185600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 104\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-27-03\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.978192304408\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.978192304408\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.978192304408\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.978192304408\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.0830748370374126\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.486225877783813\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.466965081340443\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13094639575493267\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0710779845714569\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702092683315277\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.010541505215223878\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.004756353353150189\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9467553555965423\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.015297860535793007\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1197000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1197000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 105\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 9.763846153846155\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2366.400820493698\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 90.63854360580444\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2366.400820493698\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7153.304\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998423\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 21000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1197000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 105\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-27-10\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07254844307899475\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701952719688416\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.020310158003121616\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.005728735076263547\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9652555465698243\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.014581426442600787\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1208400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1208400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 106\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.21818181818182\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2373.961475610733\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.560655117034912\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2373.961475610733\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7216.209\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998430\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 21200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1208400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 106\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-27-18\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06786025762557983\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701990902423859\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.0013262979686260224\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.017964175995439292\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8182796955108642\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01663787367288023\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1219800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1219800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 107\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 76.6\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2381.4923548698425\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.530879259109497\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2381.4923548698425\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7279.084\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998438\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 21400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1219800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 107\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-27-26\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0692575752735138\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7018429160118105\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.025860750628635288\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: -0.008634394826367497\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8601778388023377\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01722635147161782\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1231200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1231200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 108\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 76.02727272727273\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2389.0271508693695\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.5347959995269775\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2389.0271508693695\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7333.543\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 21600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1231200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 108\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-27-33\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06910364627838135\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701832604408264\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.0366529893130064\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.0550034036161378\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -1.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.018350414093583824\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1242600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1242600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 109\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 76.33636363636364\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2396.6922955513\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.665144681930542\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2396.6922955513\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7395.605\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998453\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 21800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1242600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 109\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-29-04\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.98243503266\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.98243503266\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.98243503266\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.98243503266\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08313361481176205\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.48812847342189\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.467370526835204\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13094348617898924\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06689298748970032\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702178263664246\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.00026281801983714105\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.014348294353112578\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.9510356903076171\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01408547640312463\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1254000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1254000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 110\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 10.179230769230768\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2487.3657834529877\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 90.67348790168762\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2487.3657834529877\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7447.676\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 22000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1254000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 110\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-29-12\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.0719897210597992\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701737642288208\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.0033229249063879253\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.013925032201223075\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.4824249744415283\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.017247961740940808\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1265400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1265400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 111\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.97272727272727\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2495.1018018722534\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 7.736018419265747\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2495.1018018722534\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7497.365\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998552\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 22200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1265400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 111\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-29-20\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07184103727340699\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701851081848145\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.021687474194914104\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.03903012203518301\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8411902010440826\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01734264742117375\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1276800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1276800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 112\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.08333333333333\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2503.1566500663757\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 8.054848194122314\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2503.1566500663757\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7568.492\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998560\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 22400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1276800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 112\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-29-29\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07636551856994629\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701776099205017\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.005751155852340162\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.02308030768763274\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.8409002244472503\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.01732915083412081\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1288200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1288200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 113\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.08181818181819\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2511.3158464431763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 8.159196376800537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2511.3158464431763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7650.824\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998569\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 22600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1288200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 113\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-29-37\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.97315553932\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.9887266544\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.07715104818344116\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701983869075775\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.004486148757860064\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.011250787251628935\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.4372405901551247\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.015736934589222075\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1299600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1299600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.84166666666667\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14937924066207312\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.757251492465063\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.0371199922039835\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28843924783326763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2519.526756286621\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 8.210909843444824\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2519.526756286621\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7709.106\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998577\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 22800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1299600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180298)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180239)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180392)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180373)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180260)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180340)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180302)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180355)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180242)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180257)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180243)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180244)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180316)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180264)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180354)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180262)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180265)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180282)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180291)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180320)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180394)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180379)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180356)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180360)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180378)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180335)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180376)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180318)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180321)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180240)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180245)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180241)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180338)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180284)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180261)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180280)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180266)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180281)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180319)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180317)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180357)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180299)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180353)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180334)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180358)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180375)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180263)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180359)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180377)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180246)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180301)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180300)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180374)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180303)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180283)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180336)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=180279)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-31-06\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.98068375763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.98068375763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.98068375763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.98068375763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08313997977896387\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.489303681662956\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.467120402738454\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13095124631047964\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06360703110694885\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.702024245262146\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.019352753181010485\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 52.40536839962006\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.00019672363996505737\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 52.42472131252289\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1311000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1311000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 115\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 8.70236220472441\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2608.3117825984955\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 88.78502631187439\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2608.3117825984955\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 7562.721\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998666\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 23000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1311000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 115\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-31-15\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.055272459983825684\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.700718128681183\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.030208217026665807\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 99.32593669891358\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.00020397305488586425\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 99.29572734832763\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1333800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1333800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 117\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 71.60000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2616.8596341609955\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.297013282775879\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2616.8596341609955\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6907.467\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998675\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 23400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1333800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 117\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-31-24\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05503128170967102\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.70096344947815\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.021799918683245777\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 98.18113441467285\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -2.391636371612549e-06\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 98.15933399200439\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1356600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1356600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 119\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.41666666666667\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2625.70227599144\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.453423500061035\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2625.70227599144\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 6271.08\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998684\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 23800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1356600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 119\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-32-45\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.984972908596\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.984972908596\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.984972908596\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.984972908596\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08295871773987433\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.476374332303486\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4612032059437086\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13060475595200366\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05488765835762024\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701018071174621\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.005264657735824585\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 95.2579833984375\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 2.7279555797576905e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 95.26324672698975\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1368000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1368000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 120\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 8.185470085470083\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2707.3630096912384\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 81.66073369979858\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2707.3630096912384\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5961.674\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998765\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 24000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1368000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 120\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-32-55\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05502314567565918\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701746737957\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.03120117145590484\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 93.5238245010376\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0005711123347282409\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 93.49262390136718\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1390800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1390800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 122\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.14285714285714\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2716.7305421829224\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.661226272583008\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2716.7305421829224\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5317.888\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998775\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 24400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1390800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 122\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-33-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.059237951040267946\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701200330257416\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.017548034735955297\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 93.45895557403564\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.00022511482238769532\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 93.44140796661377\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1413600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1413600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 124\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 71.11428571428571\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2726.3264725208282\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.8996901512146\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2726.3264725208282\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4640.806\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998785\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 24800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1413600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 124\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-34-33\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.98492964762\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.98492964762\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.98492964762\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.98492964762\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08298189177932168\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.47827290244026\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4615031171112736\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13061264828191432\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05914246439933777\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701408529281617\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.02529966433066875\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 83.9883321762085\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.00079430490732193\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 83.96303310394288\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1425000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1425000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 125\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 7.794444444444443\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2814.1675012111664\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 87.84102869033813\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2814.1675012111664\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4528.75\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998873\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 25000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1425000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 125\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-34-38\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06104089021682739\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701502823829651\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.008280270989052952\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 55.920932388305665\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0005958601832389832\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 55.912652015686035\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1436400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1436400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 126\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 70.78571428571429\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2819.2719757556915\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.1044745445251465\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2819.2719757556915\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4612.992\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998878\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 25200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1436400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 126\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-34-43\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05519992113113403\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701587045192719\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.016004629014059902\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 89.94355640411376\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.00396612286567688\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 89.92755165100098\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1447800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1447800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 127\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.07142857142857\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2824.226815223694\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 4.954839468002319\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2824.226815223694\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4678.79\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998883\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 25400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1447800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 127\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-34-48\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05540788173675537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.7013926148414615\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.011340011819265782\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 66.35081930160523\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.0032292842864990235\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 66.33948059082032\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1459200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1459200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 128\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.6\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2829.2472331523895\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.020417928695679\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2829.2472331523895\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4741.568\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998888\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 25600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1459200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 128\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-34-54\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05832504034042359\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701502108573914\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.0032552022952586413\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 33.67674727439881\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.004035823047161102\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 33.673492431640625\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1470600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1470600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 129\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 70.64285714285714\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2834.368432998657\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.1211998462677\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2834.368432998657\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4808.077\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998894\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 25800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1470600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 129\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-36-22\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.98101374993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.98101374993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.98101374993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.98101374993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08301217191870991\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.480126450488705\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.4618926266014207\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13063329662308554\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.05884934067726135\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701045024394989\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.04300579335540533\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 38.746705532073975\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.004190024733543396\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 38.78971138000488\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1482000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1482000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 130\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 8.165079365079364\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2922.609456539154\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 88.24102354049683\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2922.609456539154\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4881.445\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998982\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 26000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1482000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 130\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-36-27\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06024191379547119\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.701128387451172\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.004406872461549938\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 17.527905058860778\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.0071323603391647335\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 17.523498034477235\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1493400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1493400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 131\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 66.0875\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2927.9560720920563\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.346615552902222\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2927.9560720920563\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 4943.703\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998987\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 26200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1493400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 131\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-36-33\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06415757536888123\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.70102082490921\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.007674789754673839\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 16.814124631881715\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.00020904988050460816\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 16.806450176239014\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1504800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1504800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 132\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.23750000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2933.3147723674774\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.358700275421143\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2933.3147723674774\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5013.851\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 26400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1504800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 132\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-36-39\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06392456293106079\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.700675857067108\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.013290382223203778\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 6.505053591728211\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.0017356544733047486\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 6.518343937397003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1516200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1516200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 133\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 71.5625\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2938.9002537727356\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.585481405258179\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2938.9002537727356\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5102.309\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651998999\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 26600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1516200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 133\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-36-44\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06076788902282715\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.700652050971985\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.008277252502739429\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 5.0589011490345\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.015516816079616547\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 5.067178416252136\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1527600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1527600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 134\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 72.36250000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 2944.495840072632\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.59558629989624\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 2944.495840072632\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5172.583\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651999004\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 26800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1527600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 134\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=182429)\u001b[0m I have finished the episode\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-38-13\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   evaluation:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_len_mean: 11446.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_max: -9999.971046426477\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_mean: -9999.971046426477\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episode_reward_min: -9999.971046426477\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     episodes_this_iter: 1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     hist_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_lengths:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - 11446\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       episode_reward:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       - -9999.971046426477\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_action_processing_ms: 0.08303817548711807\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_env_wait_ms: 4.4815552248420465\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_inference_ms: 2.46228997569574\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       mean_raw_obs_processing_ms: 0.13065906710959868\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     timesteps_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.057248806953430174\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.700148046016693\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.020350650371983647\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 3.4695650696754456\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.05046080648899078\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 3.449214428663254\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1539000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1539000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 135\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 8.363492063492064\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.200000000000003\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 3032.900139570236\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 88.40429949760437\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 3032.900139570236\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5230.133\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651999093\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 27000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1539000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 135\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-38-19\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06720681190490722\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.700306534767151\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.03502117763273418\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 4.901746279001236\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.027999913692474364\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 4.8667251646518705\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1550400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1550400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 136\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 67.92500000000001\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 3038.473751783371\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.573612213134766\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 3038.473751783371\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5278.048\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651999099\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 27200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1550400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 136\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-38-24\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.050258868932724\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.699963295459748\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: -0.013961297902278602\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 2.267006739974022\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: 0.011206459999084473\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 2.2809680074453356\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1561800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1561800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 137\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.4\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 3044.13910984993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.665358066558838\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 3044.13910984993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5348.127\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651999104\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 27400\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1561800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 137\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-38-30\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.059522098302841185\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.700357043743134\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.03132356668356806\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.6666914165019989\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.06617964655160904\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.6353678472340107\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1573200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1573200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 138\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 74.275\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 3049.9175300598145\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.7784202098846436\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 3049.9175300598145\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5423.234\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651999110\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 27600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1573200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 138\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m Result for DDPPO_SimpleTradingEnv-training-V01_08bcc_00000:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   agent_timesteps_total: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   date: 2022-05-08_10-38-36\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_len_mean: 11445.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_media: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_max: -9999.948698479382\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_mean: -9999.983111120166\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episode_reward_min: -9999.99533320993\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_this_iter: 0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   episodes_total: 114\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   experiment_id: edb62e4ac3a4453db0db0cb02b2df68e\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   hostname: i1\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   info:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learner:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m       default_policy:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         custom_metrics: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         learner_stats:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           allreduce_latency: 0.06112541556358338\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_kl_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           cur_lr: 1.0e-05\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy: 5.699902594089508\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           entropy_coeff: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           kl: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           policy_loss: 0.016957989288493992\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           total_loss: 0.16804647403769196\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_explained_var: -0.04930619597434997\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m           vf_loss: 0.15108847934752703\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         model: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m         num_agent_steps_trained: 50.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_sampled: 1584600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained: 1584600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     num_steps_trained_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   iterations_since_restore: 139\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   node_ip: 192.168.178.59\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   num_healthy_workers: 57\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   off_policy_estimator: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     cpu_util_percent: 73.1888888888889\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     ram_util_percent: 9.2\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   pid: 179411\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_max: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_mean: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   policy_reward_min: {}\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   sampler_perf:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_action_processing_ms: 0.14944722320440537\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_render_ms: 0.0\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_env_wait_ms: 9.836046976486882\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_inference_ms: 4.035977330874926\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     mean_raw_obs_processing_ms: 0.28891607371071704\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_since_restore: 3055.7725627422333\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_this_iter_s: 5.855032682418823\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   time_total_s: 3055.7725627422333\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timers:\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m     learn_time_ms: 5496.269\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timestamp: 1651999116\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_since_restore: 27800\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_this_iter: 200\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   timesteps_total: 1584600\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   training_iteration: 139\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   trial_id: 08bcc_00000\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   warmup_time: 16.103707313537598\n",
      "\u001b[2m\u001b[36m(run pid=177576)\u001b[0m   \n"
     ]
    }
   ],
   "source": [
    "from ray.tune import JupyterNotebookReporter\n",
    "\n",
    "jupyter_reporter = JupyterNotebookReporter(True, max_report_frequency=30)\n",
    "\n",
    "analysis = tune.run(\n",
    "    run_or_experiment=\"DDPPO\",  \n",
    "    name=\"Bitcoin_Acc_Local_DDPPO_Trial_V02\",\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    stop={\n",
    "        # An iteration is equal with one SGD round which in our case is equal to (number of workers multiplied by the environment length).\n",
    "        \"training_iteration\": 1000, \n",
    "        # \"timesteps_total\": 0, # kept as reference\n",
    "        \"episode_reward_mean\": 25000 # buy and hold value for X_valid dataset, if get here we should stop and train only the generation that beats it, moreover we can hypertune that one particulary.\n",
    "        \n",
    "    },\n",
    "    config=ddppo_trainer_config,\n",
    "    num_samples=1,  # Have one sample for each hyperparameter combination. You can have more to average out randomness.\n",
    "    keep_checkpoints_num=50,  # Keep the last X checkpoints\n",
    "    checkpoint_freq=5,  # Checkpoint every X iterations (save the model)\n",
    "    local_dir=\"/tmp/ray_results/\",  # Local directory to store checkpoints and results, we are using tmp folder until we move the notebook to a docker instance and we can use the same directory across all instances, no matter the underlying OS\n",
    "    progress_reporter=jupyter_reporter,\n",
    "    #restore=\"/tmp/ray_results/SimpleTrading_Env_Distributed_PPO_Trial_V08/PPO_SimpleTradingEnv-training-V01_687da_00000_0_2022-04-29_17-26-50/checkpoint_000711\",\n",
    "    fail_fast=\"raise\",\n",
    "    resume=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\", scope=\"all\") \n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial, metric=\"episode_reward_mean\")\n",
    "\n",
    "# manual_checkpoint = analysis.get_last_checkpoint()\n",
    "# manual_trial = analysis.trials[manual_checkpoint.trial_id]\n",
    "\n",
    "print(\"Best checkpoint:\", best_checkpoint)\n",
    "print(\"Best trial:\", best_trial)\n",
    "print(\"Trial result\", best_trial.last_result[\"episode_reward_mean\"])\n",
    "# print(\"Manual checkpoint:\", manual_checkpoint)\n",
    "# print(\"Manual trial:\", manual_trial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e489d",
   "metadata": {},
   "source": [
    "### Evaluate trained model restoring it from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27386b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents import ppo\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n",
    "\n",
    "agent = ppo.PPOTrainer(config=best_trial.config)\n",
    "agent.restore(best_checkpoint)\n",
    "\n",
    "results_table = []\n",
    "json_dict_list = []\n",
    "\n",
    "\n",
    "exploration_types = [True] # True for stochastic, False for deterministic\n",
    "envs = [eval_env, training_env]\n",
    "\n",
    "for iter, env in enumerate(envs):\n",
    "    for exploration_type in exploration_types:\n",
    "        episodes_to_run = 1 if exploration_type else 1 # 5 episodes for stochastic exploration to average out randomness, 1 for deterministic\n",
    "        \n",
    "        for i in range(episodes_to_run):\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            obs = eval_env.reset() # we are using the evaluation environment for evaluation\n",
    "            net_worths = []\n",
    "            while not done:\n",
    "                action = agent.compute_single_action(obs, explore=exploration_type) # stochastic evaluation, it's not deterministic and seems to be the best for PPO training\n",
    "                obs, reward, done, info = eval_env.step(action)\n",
    "\n",
    "                net_worths.append(info['net_worth']) # Add all historical net worths to a list to print some statistics at the end of the episode\n",
    "                episode_reward += reward\n",
    "\n",
    "            results_table.append([i, \"evaluation\" if iter == 0 else \"training\" ,exploration_type, episode_reward, (episode_reward/len(net_worths)),net_worths[-1], np.mean(net_worths), \n",
    "            np.std(net_worths), np.max(net_worths), np.min(net_worths)])\n",
    "\n",
    "            tba = {\"environments\" : \n",
    "                {\n",
    "                    \"name\": \"evaluation\" if iter == 0 else \"training\",\n",
    "                    \"explorationType\": \"stochastic\" if exploration_type else \"deterministic\",\n",
    "                    \"episodes\": [\n",
    "                        {\n",
    "                            \"episode\": i,\n",
    "                            \"reward\": episode_reward,\n",
    "                            \"accumulatedEpisodeReward\": episode_reward,\n",
    "                            \"episodeRewardMean\": episode_reward/len(net_worths),\n",
    "                            \"netWorthEndOfEp\": net_worths[-1],\n",
    "                            \"netWorthMean\": np.mean(net_worths),\n",
    "                            \"netWorthStd\": np.std(net_worths),\n",
    "                            \"netWorthMax\": np.max(net_worths),\n",
    "                            \"netWorthMin\": np.min(net_worths)\n",
    "                        }\n",
    "                    ]\n",
    "            }}\n",
    "\n",
    "            json_dict_list.append(tba)\n",
    "\n",
    "\n",
    "\n",
    "headers=[\n",
    "    \"Episode\",\n",
    "    \"Env\",\n",
    "    \"Stochastic\",\n",
    "    \"Acc. ep. reward end of the eval ep\",\n",
    "    \"Avg. ep. reward\", \n",
    "    \"Net worth end of the eval ep.\",\n",
    "    \"Avg. net worth over the eval ep.\",\n",
    "    \"Std of net worth over the eval. ep.\",\n",
    "    \"Max net worth\",\n",
    "    \"Min net worth\",\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "print(tabulate(results_table, headers))\n",
    "\n",
    "# Write the results to JSON file\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(json_dict_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa6f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 18:36:56.561690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-28 18:36:56.627003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-28 18:36:56.627569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TensorTrade.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
